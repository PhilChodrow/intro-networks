# Network Exploration

Suppose that we have a very large network, and we'd like to explore it. Perhaps we don't even have access to all the edges. This is a common situation in data collection. A webscraper for gathering data, for example, doesn't have access to the entire network of webpages to be scraped; usually it has access to *one page at a time* and the links it contains. At each stage, the webscraper needs to follow a link in order to get the next piece of information. 

There are many ways to explore a network, but the most fundamental one is the *simple random walk*. We'll discuss the simple random walk, highlight some of its core properties, and connect it to some ideas that we've already seen. Then, we'll introduce the *teleporting random walk*, which is the foundation of the internet-changing PageRank algorithm originally used by Google. Finally, we'll take another look network navigation and the small-world phenomenon. 

## The Simple Random Walk {#sec-srw}

The simple random walk is easier to describe in English than it is to formulate mathematically. In intuitive terms, we imagine a walker who starts at some node $i$ in the graph. The walker then picks one of the neighbors $j$ of $i$ uniformly at random, and moves to $j$. Then, the walker picks one of the neighbors $k$ of $j$ uniformly at random, and moves to $k$, and so on. 

::: {.callout-note}
::: {#def-simple-random-walk}

## Simple Random Walk (SRW)

A **simple random walk** on a graph $G$ with adjacency matrix $\mathbf{A}\in \mathbb{R}^{n\times n}$ is a countable sequence of random variables $X_1,\ldots,X_t,\ldots$ which take values in the node set $N$ of $G$. We have 
$$
\begin{aligned}
\mathbb{P}(X_{t+1} = i | X_t = j_t, X_{t-1} = j_{t-1},\ldots,X_{1} = j_1) &=  \mathbb{P}(X_{t+1} = i | X_t = j_t) \\ 
&= \begin{cases}
        \frac{1}{d_j} &\quad j \text{ is connected to } i \\ 
        0 &\quad \text{otherwise.}
    \end{cases} \\ 
    &= \frac{a_{ij}}{d_j}\;. 
\end{aligned}
$$
:::
:::

In @def-simple-random-walk, the first equality is the *[Markov property](https://en.wikipedia.org/wiki/Markov_property)*: the future state of the random walk depends only on where it is right now (i.e. the value of $X_t$), not where it's been previously. The second and third equalities give mathematical structure to the idea of "picking a node connected to $j$ uniformly at random." 

[Note: relative to Problem 3 in HW0, I have switched the indices in this matrix in order to better align with Newman's discussion in 6.14.3.]{.aside}

::: {.callout-note}
::: {#def-transition-matrix}

## Transition Matrix of a SRW

The **transition matrix** of a simple random walk on a graph $G$ with adjacency matrix $\mathbf{A}$ is $\mathbf{P} = \mathbf{A}\mathbf{K}^{-1}$. Its entries are $p_{ij} = a_{ij}/k_j$, where $k_j$ is the degree of node $j$ and $\mathbf{K}$ is the diagonal matrix whose $jj$th entry is $k_j$. 
:::
:::

::: {.callout-caution}
***Exercise***: Let $p_{ij}$ denote the entries of $\mathbf{P}$. Check that 
$\sum_{i \in N} p_{ij} = 1$, and interpret this fact. 
:::

What does the transition matrix do? Well, we can use the law of total probability to write: 
$$
\begin{aligned}
\mathbb{P}(X_{t+1} = i) &= \sum_{j \in N}\mathbb{P}(X_{t+1} = i|X_{t} = j)\mathbb{P}(X_t = j) \\ 
&= \sum_{j \in N}p_{ij}\mathbb{P}(X_t = j)\;.
\end{aligned}
$$

Let $\mathbf{q}(t)$ be the vector with entries $q_i(t) = \mathbb{P}(X_{t} = i)$. We have just shown the most important relation in the study of random walks and their generalizations, Markov chains: 

:::{.callout-tip}
$$
\mathbf{q}(t+1) = \mathbf{P}\mathbf{q}(t)
$${#eq-transition}
:::

@eq-transition has an immediate consequence: $\mathbf{q}(t) = \mathbf{P}^{t}\mathbf{q}(0)$. So, if we know the *initial distribution* $\mathbf{q}(0)$ describing the location at which the walker begins, we can get all other information we need from the matrix $\mathbf{P}$ and its powers. 

### Stationary Distribution: Existence and Uniqueness

Throughout this section, we assume that the graph $G$ is connected. 

:::{.callout-note}
We say that an SRW has a *stationary distribution* $\pi \in \mathbb{R}^n$ if $\lim_{t\rightarrow \infty} \mathbf{q}(t) = \mathbf{\pi}$, *regardless of the initial distribution* $\mathbf{q}(0)$. 
:::

:::{.callout-note}
:::{#def-periodic}

A graph is *aperiodic* if the greatest common divisor of lengths of its cyclse is 1. A graph is *periodic* if it is not aperiodic. 
:::
:::

A simple example of a periodic graph is a $k$-cycle. 
A less simple example is a *bipartite graph*, in which each node can be separated into one of two classes $S_1$ and $S_2$ such that nodes in $S_1$ only connect to nodes in $S_2$ (and not to any other nodes in $S_1$). 
You can check that bipartiteness implies that every cycle has length divisible by 2, so the graph is periodic. 

:::{.callout-tip}
:::{#thm-stationary}
Suppose that $G$ is connected and aperiodic. Then, 

- $G$ has a stationary distribution $\pi$. 
- This stationary distribution is unique. 
- The stationary distribution is the unique eigenvector of the transition matrix $\mathbf{P}$ with eigenvalue $1$. 
:::
:::

@thm-stationary is actually a theorem about discrete-time finite-state Markov chains in general, and its full proof is beyond the scope of this course. Here's a sketch:

1. The condition that $G$ is connected implies that the matrix $\mathbf{P}$ is [irreducible](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem#Classification_of_matrices).
2. This allows us to apply the [Perron-Frobenius theorem](https://en.wikipedia.org/wiki/Perronâ€“Frobenius_theorem) to conclude that $\mathbf{P}$ has a unique eigenvector with strictly positive entries, which we'll call $\pi$. Since $\mathbf{P}$ has rows that sum to 1, a direct calculation shows that the corresponding eigenvector of $\pi$ is unity, and we have $\pi = \mathbf{P}\pi$. 
3. One further shows that $\lim_{t \rightarrow \infty}\mathbf{P}^t$ exists and that its rows become equal to the eigenvector $\pi$ (this is related to power iteration). 
4. Because each row of limiting matrix describe the long-run behavior of the random walk from a given starting node, one infers that $\pi$ is indeed a stationary state. Because all the rows agree in the limit, one infers that $\pi$ is the only such stationary state. 

### Stationary Distribution: Structure

We know from @thm-stationary that there is a unique stationary distribution for the simple random walk, and that this distribution describes, in the long-term, the amount of time that the walker spends on a node. What actually *is* the stationary distribution? For many Markov chains, including more complicated random walks, it can be difficult to describe the stationary distribution exactly. In this case the answer is surprisingly simple, although it's one of those things that's much easier to check than it is to find in the first place. 

:::{.callout-note}
:::{#thm-degree}
## Stationary Distribution of an SRW

The stationary distribution of an SRW is $\pi = \frac{1}{2m}\mathbf{k}$, where $\mathbf{k}$ is the vector of node degrees. 
:::
:::

:::{.proof}
We can do a direct calculation:

$$
\begin{aligned}
\mathbf{P}\pi = \mathbf{A}\mathbf{K}^{-1}\left(\frac{1}{2m}\mathbf{k}\right) = \frac{1}{2m}\mathbf{A}\mathbf{1} 
= \frac{1}{2m}\mathbf{k}
= \pi\;. 
\end{aligned}
$$

For the second equality we have used the quickly-checked identity $\mathbf{K}^{-1}\mathbf{k} = \mathbf{1}$, while for the third we have used the formula $\mathbf{k} = \mathbf{A}\mathbf{1}$. 
:::

So, for simple random walks, $\mathbb{P}(X_t = i)$ when $t$ is large is approximately $k_i/2m$, proportional to the degree of node $i$. The intuition here is that the amount of time I spend in state is directly proportional to the number of ways that I can enter that state. 

## Exploration and Ranking: PageRank

Coming soon!








