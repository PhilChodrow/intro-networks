[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 168: Introduction to Networks",
    "section": "",
    "text": "Welcome! This is the course website for MATH 168: Introduction to Networks at UCLA, as taught by Dr. Phil Chodrow, in Spring Quarter of 2022. Its primary purpose is to house lecture notes and other course resources."
  },
  {
    "objectID": "index.html#network-science",
    "href": "index.html#network-science",
    "title": "Math 168: Introduction to Networks",
    "section": "Network Science",
    "text": "Network Science\nWhat is network science? At a high level,\n\nNetwork science is the interdisciplinary study of how systems are connected and what their connection structure implies about their behavior or function.\n\nHere’s an example of a simple network. The dots represent individuals, and are called nodes. The lines represent connections, and are called edges.\n\n\nCode\n# R code\nlibrary(tidygraph)\nlibrary(ggraph)\n\ng <- play_islands(2, 10, 0.7, 2)\n\ng %>% \n  ggraph() + \n  geom_edge_link(alpha = 0.5) + \n  geom_node_point(size = 7, pch = 21, fill = \"#73b9ee\", color = \"black\") + \n  theme_void()\n\n\n\n\n\nA network with 20 nodes. This network displays one of many common properties in real-world networks, called community structure. In this case, the network is clustered into two densely interconnected ‘communities’ that are only loosely connected to each other. Community structure often accompanies phenomena such as echo chambers in online media.\n\n\n\n\nHere are some of the questions that a network scientist might ask about this network:\n\nWhat, exactly, do the nodes represent? Are they human agents? Are they concepts? Are they locations in physical space?\nWhat, exactly, do the edges represent? Are they persistent friendships in a social network? Are they intellectual connections between concepts? Are they transportation routes between locations?\nVisually, it looks like this network might be roughly divisible into two interesting pieces. Is that really true, or is that just an artifact of how we’ve drawn the network? If that is really true, what kinds of algorithms can we use to help us find these pieces automatically?\nIf we imagine a dynamical process unfolding on this network, such as information propagation or disease spread, how might the behavior of that process be influenced by the structure of the network?\nIf we were to disrupt this network by removing a node or an edge, how might its functionality change? (This depends very strongly on what the nodes and edges represent.)"
  },
  {
    "objectID": "index.html#this-course",
    "href": "index.html#this-course",
    "title": "Math 168: Introduction to Networks",
    "section": "This Course",
    "text": "This Course\nIn this course, we’ll study some of the primary questions we can ask about networks from a mathematical point of view. These questions include:\n\nWhat kind of things is it useful to measure in networks?\nWhat are some properties shared by many real-world networks?\nWhat mathematical models are useful for understanding some of these properties?\nWhat kinds of algorithms can we use to extract insights from network data?\n\nAlong the way, we’ll build our mathematical toolboxes, especially those related to linear algebra and probability."
  },
  {
    "objectID": "index.html#figure-code",
    "href": "index.html#figure-code",
    "title": "Math 168: Introduction to Networks",
    "section": "Figure Code",
    "text": "Figure Code\nThis site includes some figures, such as the one above, that I have generated with code. You can click the “Code” button right above the figure to view the code that created it. This code will often be in R, although I will make some effort to demonstrate some techniques with Python and NetworkX as well."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "I’m Dr. Phil Chodrow, a visiting assistant professor in the Department of Mathematics at UCLA. My pronouns are he/him/his. I grew up in Virginia, did undergrad at Swarthmore College in Pennsylvania, and did my PhD (after a few years traveling and working) at MIT. Then I came here to UCLA!\nI love applied math, ethical data science, Star Trek, penguins, cooking, tea, Studio Ghibli movies, traditional martial arts, and effective pedagogy.\n\n\nIf you’re not sure, please call me “Professor Chodrow.” I usually invite Learning Assistants and student research collaborators to address me as “Phil.”\nPlease remember to address all your professors respectfully and according to their preferences. As argued in a recent study, many of us have harmful, gendered biases about when we use earned titles like “Dr.” or “Professor.” A small, simple thing you can do to make academia a more equitable place is to check your own potential biases. If you’re not sure, “Professor X” or “Dr. X” is always a safe choice — but even better is to just ask what your instructor prefers! My own personal preference is related to this short poem by Susan Harlan.\n\n\n\nI’ve collected a bunch of FAQs about myself and various things that don’t quite fit into a course syllabus here."
  },
  {
    "objectID": "syllabus/syllabus.html#guiding-principles",
    "href": "syllabus/syllabus.html#guiding-principles",
    "title": "Syllabus",
    "section": "Guiding Principles",
    "text": "Guiding Principles\n\nI want you to succeed. The purpose of this course is for you to build a set of analytical tools for thinking critically about the many connected systems we encounter in the modern world. Along the way, you’ll learn some new mathematics and build some interesting, creative projects. I view it as part of my job to help you along the way. I’m not succeeding as a teacher unless you are succeeding as a student. I will do my best to proactively remove barriers to your learning in this course, and I hope that you’ll communicate with me if you see opportunities for me to help you out.\nIt’s still tough out there. Although some parts of our lives might be returning to normal, other parts remain affected by the COVID-19 pandemic. My main aims in designing this course are to (a) offer you flexibility to adapt to changing circumstances and (b) encourage you to support and be supported by your classmates. Network science is fun. I hope that this course can be a positive part of your experience during these challenging times.\nYour wellbeing comes first. If your wellbeing or that of a loved one comes into conflict with course obligations, I hope that you will prioritize the former. I’ve included a considerable amount of flexibility in this course. If you anticipate extended difficulties related to participation or assignments, reach out to me at the earliest opportunity. We’ll find a path that prioritizes your wellbeing while still enabling you to succeed in the course. I’ve given some examples below about some situations in which I hope you’ll reach out.\nWe’ve got your back. As the instructor, I’m available to you through multiple channels. Your TA and peers are all here to help you in your learning journey."
  },
  {
    "objectID": "syllabus/syllabus.html#learning-objectives",
    "href": "syllabus/syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this course:\n\nYou will relate mathematical definitions of network measures to intuitive, English-language descriptions of their meanings.\nYou will use “large graph limit” arguments to reason about the qualitative behavior of random and real-world graphs.\nYou will write programs to compute network measures, sample from random network models, simulate processes evolving on networks, and perform network data science tasks.\nYou will write both formal proofs and heuristic arguments to prove properties of network algorithms and models.\nYou will read and summarize classical and recent research papers in several distinct areas of network science.\nBeyond math: You will analyze, reflect on, and write about questions of bias, fairness, and justice in networked settings.\n\nBeyond math: You will describe the abstractions and accompanying limitations of network models as descriptions of real-world phenomena.\n\n\nOfficial Course Description\nIntroduction to network science (including theory, computation, and applications), which can be used to study complex systems of interacting agents. Study of networks in technology, social, information, biological, and mathematics involving basic structural features of networks, generative models of networks, network summary statistics, centrality, random graphs, clustering, and dynamical processes on networks. Introduction to advanced topics as time permits. P/NP or letter grading.\n\n\nYour Preparation\nThe official prerequisites for MATH 168 are upper-division linear algebra (MATH 115A) and upper-division probability (MATH 170E or similar). I am expecting you to be able to write proofs and arguments that rely on material covered in these classes. That said, I don’t need you to be ready to take a 115A exam at a moment’s notice.\nBriefly, my expectation is that, when you have access to books, notes, and the internet, you won’t get stuck on problems involving linear algebra or probability content. In a bit more detail:\n\nWhen confronted with a problem whose solution requires linear algebra or probability, you are able to:\n\nIdentify the general topic needed for the problem (e.g. “eigenvalues of matrices,” “Chebyshev’s inequality”).\nRapidly identify where you need to look to find the result that you need in books, previous course notes, or online.\nRecognize and use this result in a careful, insightful, and correct way in your solutions.\n\n\nI’ll also expect you to write code to perform simulations, experiments, and data analyses. There will be coding problems in homework, and you are also likely to want to write code for your course project. Python, R, and Julia are all good choices. If you don’t have prior experience in computing, my recommendation is Python. I have gathered some Python resources to help you get started, and we’ll see some additional examples in lecture and discussion.\n\n\nCourse Environment\n\nDiversity You deserve to be welcomed and celebrated by our community. We embrace diversity of age, background, beliefs, ethnicity, gender, gender identity, gender expression, national origin, religious affiliation, sexual orientation, and other visible and non-visible categories. Discrimination is not tolerated in my classroom.\n\n\nTitle IX You deserve a learning environment free from discrimination, sexual harassment, sexual assault, domestic violence, dating violence, and stalking. If you experience these behaviors or otherwise know of a Title IX violation, you have many options for support and/or reporting. The UCLA Title IX Office can help you navigate your options.\n\n\nAccessibility You deserve to fully and equitably participate in our learning environment. I am actively putting effort into ensuring that course materials are screen-reader accessible, and welcome feedback on where I can do better. The UCLA Center for Accessible Education and Disabilities and Computing Program may be able to help remove barriers to learning.\n\n\nNames and Pronouns  You deserve to be addressed in the manner that reflects who you are. I welcome to tell me your pronouns and/or preferred name at any time, via Zoom, in person, or via email. Conversely, please address your classmates according to their correct pronouns."
  },
  {
    "objectID": "syllabus/syllabus.html#course-details",
    "href": "syllabus/syllabus.html#course-details",
    "title": "Syllabus",
    "section": "Course Details",
    "text": "Course Details\nInstructor: Prof. Phil Chodrow  Teaching Assistant: Grace Li\nLecture Classroom: Mathematical Sciences 6229  Time: 10am-10:50am\nDiscussion Classroom: Mathematical Sciences 6229 (same as lecture).\n\nRequired Textbook\nWe will be using the following text:\n\nMark Newman (2018), Networks, 2nd edition.(Newman 2018)\n\nPlease note that we will be studying problems and topics from the 2nd edition. The 1st edition can also be found, but if you buy it you might end up doing the wrong problems!\nWe may also supplement with additional free online resources throughout the course.\n\n\nIn-Person Learning\nI believe that you learn better and that we form a more cohesive learning journey when students attend class in person. Provided that UCLA policy encourages in-person attendance, my expectation is that most students will attend class live.\nThat said, I am also aware that some of you may be concerned about your health or the health of your loved ones. Others of you may be unable to reach campus in time for class. While I can’t offer you an experience on par with those who attend live, I do intend to make things work for you.\nConcretely, here’s how things are going to work:\n\nLectures and Discussions will take place live, at our scheduled time and in our scheduled room.\nLectures and Discussions which feature extensive presentation by me or the TA will be recorded using the in-room recording setup. Sessions that primarily feature student discussion or activities may not be recorded.\nProvided that live attendance remains high, I plan to make the recordings public to all students.\n\nThe primary purpose of posting recordings is to make it possible for students who feel unsafe to be able to participate with the course. Recordings are not intended to act as a substitute for attendance for most students. I reserve the right to restrict access to recordings if attendance during class periods drops off.\n\n\nAssessment\nYour grade in MATH 168 will be calculated using the following categories:\n\nHomework: 40 points.\nMidterm Exam: 15 points.\nCumulative Project: 50 points.\n\nYour final score in the class is the sum of your scores on homework, midterm, and cumulative project. Letter grades will then be assigned using the straight scale as a floor. For example, any final score above 90 is enough to guarantee at least an A-, but I reserve the right to grant an A instead according to my judgment.\nThe only exception to this policy is the A+ grade, which is not guaranteed by any final score. I grant these by discretion.\nYou’ll notice that there is a total of 105 possible points. That gives you 5 points which you can drop without consequence. In practice, I expect most folks to use this flexibility by skipping some homework problems. However, this is ultimately up to you.\n\n\nHomework Assignments\nWe’ll have a total of 8 homework assignments throughout the quarter. These 8 assignments will contain a total of at least 40 problems. (There may be a few more than 40 problems available, but only 40 of them will count toward your grade).\nThe computation of your overall homework score is simple: your homework score is the number of problems for which you’ve received credit, up to a maximum of 40. Have you received credit for 38 problems throughout the quarter? Then your homework score is 38/40.\n\nSpecifications Grading\nI’m going to say something that’s going to sound scary:\n\nThere is no partial credit on homework problems in MATH 168.\n\nA solution can either meet specifications, in which case it receives credit, or not yet meet specifications, in which case it does not receive credit (yet).\nInstead of partial credit, you have multiple attempts. If on your first try your solution does not meet specs, you’ll get it back from the TA with a comment on what’s in need of improvement. You can then revise and resubmit the solution. The TA will take another look: if you’ve met the specifications, you now get full credit. Nice job! One point toward your final grade in the course.\nThere will be a due date on your first submission, but you can submit revised second versions at any time prior to the end of the quarter.\nHere is a list of the standard specifications. These are the specifications that will be used in the vast majority of assigned homework problems. There may be a few problems with custom specifications, which will be supplied with the problem statement.\n\n\nLate and Partial Attempts\nPlease note that you can only resubmit a problem for full credit if you have already submitted a good effort by the stated due date. If you did not submit a first draft by the due date, or if your first draft doesn’t show sufficient progress toward the problem, then your problem will be marked “Late/Partial Attempt.” A problem with this mark will receive 0.5 points (instead of 1 point) if all specifications are met in the second submission.\nIn this policy, the phrase “sufficient progress” means roughly that you are at least half-way toward a solution. The judgement of whether a first submission demonstrates sufficient progress is fully at the discretion of the TA.\nThe purpose of this policy is to keep you moving at a good speed and to save the TA from large piles of submissions and resubmissions at the end of the quarter.\n\n\nThe Resubmission Process\nSo you got back your first assignment, and you didn’t get credit on a few problems. What should you do?\n\nBreathe. I know it feels like getting a 0, and you’re not used to that! But, you still have the opportunity to get full credit, and you have feedback from the TA on how to do so.\nCarefully review your feedback from the TA on Gradescope. Think of the feedback as a checklist – revise those items to a high standard, and you’ll get credit.\nRevise your solutions. You only need to revise the problems for which you didn’t get credit the first time.\nYou can submit your revision at any time during the quarter, although I strongly suggest doing it quickly so that you don’t get bogged down. You should submit all revised problems on the same assignment at once. For example, if you need to revise your solutions to Problems 3 and 5 on HW0, then you should do so and submit one document containing the revisions for both of these problems.\n\nWe’ll make an announcement with the detailed mechanics of where and how to submit your revisions after HW0 comes out.\n\n\nHow Many Resubmissions?\nYou can always expect to have one opportunity to resubmit on homework problems.\nIn some cases, the TA may invite you to make a second resubmission if your first resubmission was very close but not quite there in some trivial way. For example, maybe your calculation was correct except for a factor of 2 that you forgot to cancel in the very last line.\nWhether you get more resubmissions is fully at the discretion of the TA and not subject to negotiation. Requests for more resubmissions are unlikely to receive a response from the TA.\nIf you made sufficient progress in your first submission (usually, getting “at least halfway there”), then your resubmitted problems can receive full credit when they meet all the specifications. If your problem was marked Late/Insufficient Progress, then you your problems can receive half credit when they meet all the specifications.\n\n\n\nMidterm Exam\nThe midterm exam will likely take place during Week 6 or early in Week 7. The exam is currently expected to be a 50-minute in-person exam during a class period. You’ll be permitted to bring any quantity of hand-written notes to the exam, but not any other resources.\nThis plan may change in response to evolving circumstances surrounding the pandemic.\n\n\nCumulative Course Project\nThere will be a cumulative course project that you will complete in groups over the course of the quarter. The bulk of the work for this project will take place in Weeks 5-10. There will be a number of milestones due throughout the course, including short essays, progress reports, and presentations."
  },
  {
    "objectID": "syllabus/syllabus.html#beyond-math-168",
    "href": "syllabus/syllabus.html#beyond-math-168",
    "title": "Syllabus",
    "section": "Beyond Math 168",
    "text": "Beyond Math 168\nThis section collects some considerations about “being a student” that go beyond the specific details of this particular course.\n\nThe Hidden Curriculum\nThe hidden curriculum refers to the implicit knowledge and habits—not usually taught explicitly—that students pick up “along the way” in their education. These often relate to asking for help, using available resources, and planning work. Often, students with college-educated parents are more comfortable in the hidden curriculum than first-generation college students. I’d like to make sure that everyone knows the following about my class.\n\nIt’s never wrong to ask me for help.  It is literally my job to help you succeed in this class. If at any time you’re concerned about your ability to keep up the pace, just reach out and we’ll see what we can do. I won’t always be able to give you exactly the support you request, but I will do my best. I’m more able to help you out if you approach me early, as soon as issues come up.\nYour wellbeing comes first.  If you are experiencing circumstances that make it difficult for you to complete your work for this class—especially if those circumstances are health-related—please let me know. There is flexibility already built into this course, and I’m happy to work with you when circumstances make it difficult for you to focus on your learning. “I didn’t manage my time well this week” isn’t usually a reason I’ll grant additional flexibility, but “I am sick,” “my internet is unreliable,” “I am changing housing,” etc. etc. are all appropriate.\nStudent Hours are for you.  Student Hours, also called Office Hours, are your time. Come by to ask questions, chat with me, or just work on homework. You don’t need a “reason” to come to Student Hours, and you shouldn’t worry about disturbing me. Again, it’s your time. I’ll be very happy to see you.\nYou can ask me to advocate for you.  This is most commonly related to letters of recommendation (see section “Advice and Letters of Recommendation”), but if there’s another way in which I can use my position to help you, let me know.\nIf something is hard for you, that’s ok.  Maybe you’re struggling on a problem. That’s good! I know it feels frustrating, but that is where learning happens. If you are having a hard time on a problem, please remember:\n\nYou are not the only one. I promise.\nYou are not a bad student.\nYou can still succeed in this class and in future endeavors involving programming.\nAsk for help! You’ve got me, the TA, and your classmates. We are all here for you.\n\nIt’s ok—actually, it’s awesome—to collaborate with your peers on homework.  It’s not cheating to work together on homeworks (at least in this class). Make sure to credit your collaborators at the top of your assignment and write a bit about how they contributed to your learning in each problem. You should also make sure that your submitted solution is written in your own words, and reflects your effort to understand the solution for yourself.\n\nTo expand on one of the points above: if you are a first-gen student, I especially encourage you to reach out to me. I’ll offer you what tips I can about navigating your time at UCLA.\n\n\nPrioritizing Your Wellbeing\nOne of the guiding principles of this course is that your wellbeing comes first. If your wellbeing comes into conflict with the course obligations, I hope that you’ll prioritize your wellbeing and reach out to me.\n\nIf you or someone you love is experiencing a health crisis, prioritize your wellbeing. You can use some of your drops to take a break from assignments, or you can also ask me for an extension or other accommodation.\nIf some aspect of the course is causing undue stress or anxiety, feel free to let me know. I regularly make adjustments to the course to promote student mental health while still meeting course learning objectives.\nIf you do not have reliable internet or other resources needed to access class resources, let me know and we’ll see what we can do.\nIf you are having trouble managing your time, feel free to ask me for advice. I don’t usually grant accommodations for this reason, but I may be able to help you use your time more efficiently in the future.\n\nThese examples are not exhaustive. If you are in any situation in which you feel that your obligations to MATH 168 are detrimental to your wellbeing or the wellbeing of someone you love, please consider contacting me. Please also remember that the sooner you approach me, the better I can help you.\n\n\nResearch Opportunities\nSorry y’all – I will not be taking any more student research collaborators at UCLA. I need to focus on completing my various open projects as I prepare to transition to my next position.\n\n\nAdvice and Letters of Recommendation\n\nAdvice\nI am always happy to talk with you about your future plans, including internships, REUs, and graduate school applications. Because I am a creature of the academy, I am less knowledgeable about industry jobs, although you are welcome to ask about those too.\n\n\nLetters of Recommendation\nIf you have completed a course with me or are currently enrolled, you are welcome to request a letter from me. If I feel that I am not able to write you a strong letter, I will tell you – but if you still want a letter from me, I will still write it.\nPlease keep in mind that I can write stronger letters for students whom I see more frequently, such as in lecture or office hours. If you’d like a letter, talking to me in these contexts, or scheduling another meeting time, is highly recommended.\nTo request a letter, fill out this request form! Please give me at least one month of advance notice when possible.\n\n\nWhen I Won’t Write a Letter\nAs a matter of moral principle, I will not write letters of recommendation for programs or jobs involving any of the following:\n\nPolicing (including but not limited to predictive policing, development of algorithms that predict recidivism, etc.);\nMilitary applications (such as internships at the Department of Defense or any of its international counterparts);\nWeapons manufacturing, broadly construed;\nIntelligence gathering (such as internships at the NSA, FBI, or any international counterpart).\n\nI am very happy to discuss this policy with any student who has questions. Conversations about when and how mathematics, data science, and programming should be used are lacking in our community. If you’d like to engage me in such a conversation, that would be great! However, this policy is non-negotiable. Therefore, if I refuse to write you a letter on these grounds, please know that it doesn’t reflect on your ability to succeed in PIC16B, your career potential, your worth as a person, or whether I like you.\nThe Just Mathematics Collective has compiled a list of resources for students on making ethical career decisions, which is available here. The text of this section is lightly modified from their suggested text on letter-writing."
  },
  {
    "objectID": "syllabus/specifications.html",
    "href": "syllabus/specifications.html",
    "title": "Standard Specifications",
    "section": "",
    "text": "Below, I’ve listed out the specifications that will be applied to the vast majority of homework problems in MATH 168. There may be a few problems with custom specifications. These specifications will be stated alongside the problem.\nYou should treat the specifications as a checklist. When you think you think you might be done with a problem, go down the applicable list of specifications. If you are confident that you have satisfied each expectation in each part of the problem, then you can expect to get credit for the problem. The reason is that this is exactly what the TA will be doing when grading your assignments."
  },
  {
    "objectID": "syllabus/specifications.html#specs-for-mathematical-argumentation",
    "href": "syllabus/specifications.html#specs-for-mathematical-argumentation",
    "title": "Standard Specifications",
    "section": "Specs for Mathematical Argumentation",
    "text": "Specs for Mathematical Argumentation\nThese specifications are in effect whenever a problem asks you to support a mathematical claim through proof, argument, or calculation.\n\nCorrectness\n\nEach direction in the problem statement is followed.\n\nNote: You are required to follow only directions, not hints. That said, I include the hints with the intention of making your life easier!\n\nThe overall structure is mathematically sound and supports the required result.\n\n\n\nExposition\n\nEach step is carefully justified. Resources that can always be cited include the course notes or lectures, the course text, and standard theorems in linear algebra and probability. Other sources are often acceptable with citation.\nThe proof or argument is presented using clear and engaging English prose. The proof or argument is written in English sentences. There is at least as much English text as there are mathematical symbols. Grammar and spelling errors are acceptable provided that the meaning is clear."
  },
  {
    "objectID": "syllabus/specifications.html#specs-for-computation-problems",
    "href": "syllabus/specifications.html#specs-for-computation-problems",
    "title": "Standard Specifications",
    "section": "Specs for Computation Problems",
    "text": "Specs for Computation Problems\nThese specifications are in effect whenever a problem asks you to perform and present a computational experiment.\n\nCorrectness\n\nEach direction in the problem statement is followed.\nThe results are correct or appear correct.\n\n\n\nExposition\n\nIf a figure is requested, this figure has clearly labeled axes and readable data markings.\nIf a figure is requested, there is a brief discussion of the meaning of the figure.\nThe code is supplied as part of the problem submission.\nThe code contains comments to describe what each part of the code does. Generally speaking, there should be at least one high-level comment for every 3-5 lines of code. More comments don’t hurt."
  },
  {
    "objectID": "syllabus/specifications.html#faqs",
    "href": "syllabus/specifications.html#faqs",
    "title": "Standard Specifications",
    "section": "FAQs",
    "text": "FAQs\nAm I required to use the \\(\\LaTeX\\) typsetting language for submitting my homework?  No, but this is highly encouraged! This an especially helpful thing for you to practice if you expect to continue on to graduate school. I am happy to supply \\(\\LaTeX\\) source files for the homework assignments on request.\nAm I required to use any specific software for computational problems? No, you are free to use any software that allows you to accomplish the task. That said, if you don’t have a pre-existing preference, we strongly recommend Python with the NetworkX package. The easiest way to get started is to download Anaconda Python, which includes up-to-date Python and NetworkX installations built right in.\nIf I meet specifications on two parts of a three-part problem, do I get partial credit? No, there is no partial credit on homework assignments in MATH 168. To receive credit for the problem, you need to meet specifications on all parts."
  },
  {
    "objectID": "syllabus/resources.html",
    "href": "syllabus/resources.html",
    "title": "Health, Wellbeing, and Equity",
    "section": "",
    "text": "UCLA’s Sexual Violence Index, including definitions, more thorough descriptions of available resources, and suggested safety measures.\nTitle IX Office.\nOffice of the Dean of Students.\nUCLA Police Department (UCPD)\n\n\n\nCampus Assault Resources & Education (CARE) - Crisis support available 24 hours a day: (310) 206-2465\nCounseling and Psychological Services (CAPS) - Counselors available by phone 24 hours a day: (310) 825-0768\nRape Treatment Center UCLA Medical Center Santa Monica - Support available 24 hours a day: (424) 259-7208\nStudent Legal Services - Available Monday-Friday, 9am-12pm and 1pm-5pm - $10 initial consultation fee, currently waived due to COVID-19.\n\n\n\nI am a Responsible Employee, which means that I am required by the University to report incidents of sexual harassment or sexual violence to the Title IX Coordinator. If you disclose such an incident to me, I will report it to the Title IX Coordinator. This may lead to follow-up from the Title IX office and a possible investigation into the incident. If you would like to speak to a resource who will maintain strict confidentiality, see Confidential Resources."
  },
  {
    "objectID": "syllabus/resources.html#mental-health",
    "href": "syllabus/resources.html#mental-health",
    "title": "Health, Wellbeing, and Equity",
    "section": "Mental Health",
    "text": "Mental Health\nCounseling and Psychological Services (CAPS) - Counselors available by phone 24 hours a day: (310) 825-0768"
  },
  {
    "objectID": "syllabus/resources.html#students-with-disabilities",
    "href": "syllabus/resources.html#students-with-disabilities",
    "title": "Health, Wellbeing, and Equity",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nOffice for Students with Disabilities (OSD) - Provides academic support services for students with documented permanent or temporary disabilities, at no cost to students. Example services include readers, adaptive equipment, sign language interpreters, test-taking arrangements, transportation, and much more.\nUCLA Disabilities and Computing Program (DCP) - Provides services related to accessible computing technology, including support for students and training for faculty."
  },
  {
    "objectID": "syllabus/resources.html#equity-inclusion-and-access",
    "href": "syllabus/resources.html#equity-inclusion-and-access",
    "title": "Health, Wellbeing, and Equity",
    "section": "Equity, Inclusion, and Access",
    "text": "Equity, Inclusion, and Access\nThe Office of Equity, Diversity, and Inclusion maintains a list of resources. Several highlights: - Resources for Racial Trauma - Resources on Immigration Policy Changes - Resources on Native American and Indigenous Affairs\nThe Community Programs Office offers several student-run programs aimed at the development of an inclusive and diverse community of scholars and leaders on and off campus. - The Student Retention Center offers several programs supporting students who may be experiencing difficulties in their academic or social lives. Specific populations served include students within the Afrikan diaspora, Raza students, Native and Indigenous students, Pilipino students, and Southeast Asian students. - Academic Advancement Program (AAP) Peer Learning (serving AAP students)."
  },
  {
    "objectID": "syllabus/resources.html#international-students",
    "href": "syllabus/resources.html#international-students",
    "title": "Health, Wellbeing, and Equity",
    "section": "International Students",
    "text": "International Students\nDashew Center for International Students and Scholars - List of resources related to visas, arrival in the US, housing, and other important topics."
  },
  {
    "objectID": "syllabus/resources.html#financial-stress",
    "href": "syllabus/resources.html#financial-stress",
    "title": "Health, Wellbeing, and Equity",
    "section": "Financial Stress",
    "text": "Financial Stress\n\nBruin Shelter provides a safe, supportive environment for fellow college students experiencing homelessness by fostering a collaborative effort between universities, community-based organizations, and service providers.\nThe CPO Food Shelter provides free food for any UCLA student who may be experiencing hunger and/or struggling to attain food due to financial hardships."
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "Below are the lecture notes for MATH 168. Please keep in mind that reading and class attendance are both expected and that content from lecture notes may not be sufficient to complete problem sets."
  },
  {
    "objectID": "chapters/measurement.html",
    "href": "chapters/measurement.html",
    "title": "1  Measuring Networks",
    "section": "",
    "text": "These lecture notes are based on Chapters 6 and 7 of Newman. They are a short set of highlights, and are not a substitute for actually reading these chapters! There will be content not covered in these notes that you’ll need for homework problems."
  },
  {
    "objectID": "chapters/measurement.html#networks-and-matrices",
    "href": "chapters/measurement.html#networks-and-matrices",
    "title": "1  Measuring Networks",
    "section": "1.1 Networks and Matrices",
    "text": "1.1 Networks and Matrices\n\n\n\n\n\n\n\nDefinition 1.1 (Undirected Graph) \nAn undirected graph \\(G = (N, E)\\) is a set of nodes \\(N\\) and a set of edges \\(E\\subseteq N\\times N\\). Each element of \\(E\\) is an unordered pair of nodes in \\(N\\). An element of \\(E\\) can be written \\((i,j)\\), where \\(i, j \\in N\\). In a simple undirected graph, \\(i \\neq j\\).\n\n\n\n\nWhen \\(i = j\\) for some element of \\(E\\), this is called a self-loop. Simple graphs contain no self-loops.\n\n\nSee Newman 6.2 for discussion of self-loops and other ways graphs can be non-simple.\nMatrices are fundamental tools for studying networks. Why is that? The key point is that a graph is a collection of pairwise relationships encoded by \\(E\\), and matrices are really good for describing pairwise relationships!\n\nThe Adjacency Matrix\nEasily the most fundamental of the matrices associated to a graph \\(G\\) is the adjacency matrix\n\n\n\n\n\n\n\nDefinition 1.2 (Adjacency Matrix) \nThe adjacency matrix \\(\\mathbf{A}\\) of a graph \\(G = (N,E)\\) is an \\(n \\times n\\) matrix, where \\(n = \\left|N\\right|\\). Its entries are \\[\na_{ij} =\n\\begin{cases}\n    1 &\\quad (i,j) \\in E \\\\\n    0 &\\quad \\mathrm{otherwise.}\n\\end{cases}\n\\]\n\n\n\n\nThe reason the adjacency matrix is so important is that it is a lossless representation of the graph structure – given knowledge of \\(\\mathbf{A}\\), you can fully reconstruct the graph. Not all matrices have this property.\n\nWalks\n\n\n\n\n\n\n\nDefinition 1.3 A walk of length \\(k \\geq 2\\)* in a graph is a set of edges \\((i_1,j_1), (i_2,j_2), \\ldots, (i_k,j_k)\\) with the property that \\(i_\\ell = j_{\\ell-1}\\) for each \\(2 \\leq \\ell \\leq k\\). We say that this is a walk from \\(i_1\\) to \\(j_k\\).\nA single edge \\((i,j)\\) is always considered a walk of length 1 from \\(i\\) to \\(j\\).\n\n\n\n\nA question that pops up a lot in network analysis is:\n\nHow many walks of length \\(k\\) exist between nodes \\(i\\) and \\(j\\)?\n\nThe adjacency matrix gives a concise way to address this question. First, let’s consider \\(k = 1\\). That’s just the number of edges between nodes \\(i\\) and \\(j\\), which is exactly \\(a_{ij}\\). Said another way,\n\nThe \\(ij\\) th entry of \\(\\mathbf{A}^1\\) counts the number of walks of length \\(1\\) between nodes \\(i\\) and \\(j\\).\n\nThis turns out to generalize smoothly by induction.\n\n\n\n\n\n\nThe \\(ij\\)-th entry of the matrix \\(\\mathbf{A}^k\\) contains the number of walks of length \\(k\\) from \\(i\\) to \\(j\\).\n\n\n\nHere’s a sketch of the proof. Suppose that \\(\\mathbf{W}(k)\\) is a matrix whose entry \\(w_{ij}(k)\\) contains the number of walks between nodes \\(i\\) and \\(j\\) of length \\(k\\). Then, \\(\\mathbf{W}(k+1) = \\mathbf{W}(k)\\mathbf{A}\\) has entries \\(w_{ij}(k+1)\\) containing the number of walks of length \\(k+1\\). To see this, expand the matrix product:\n\\[[\\mathbf{W}(k)\\mathbf{A}]_{ij} = \\sum_{\\ell \\in N}w_{i\\ell}(k)a_{\\ell j}\\;.\\]\n\n\ncf. Newman’s eq. 6.22\n\n\n\n\n\n\nExercise: What’s a very fast argument that this sum does indeed express the number of walks of length \\(k+1\\) from \\(i\\) to \\(j\\)?\n\n\n\nWorking through this exercise and applying induction, you can see that \\(\\mathbf{W}(k) = \\mathbf{A}^k\\), which completes the proof.\n\n\n\nA Linear Algebra Interlude\nWhat kind of information does the matrix \\(\\mathbf{A}\\) hold about the graph? Well, one answer is “all of it,” because \\(\\mathbf{A}\\) determines the graph up to permutations of node labels. But there’s a more useful answer as well. When we as about the information contained in a matrix, we often look at the eigenvalues and eigenvectors. The eigenvalues and eigenvectors of the adjacency matrix can contain some useful information about the graph structure. Let’s see an example.\nLet\n\\[\n\\mathbf{K}_n = \\left[\\begin{matrix}\n    0 & 1 & 1 & \\cdots & 1 \\\\\n    1 & 0 & 1 & \\cdots & 1 \\\\\n    1 & 1 & 0 & \\cdots & 1 \\\\\n    \\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\\n    1 & 1 & 1 & 0 \\cdots & 0\n\\end{matrix}\\right]\\;.\n\\]\nThere are \\(n\\) rows and \\(n\\) columns.\nThe is the adjacency matrix of an n-clique: a graph on \\(n\\) nodes in which all nodes are connected to each other.\nLet’s now consider the matrix \\[\n\\mathbf{A}_{2n} = \\left[\\begin{matrix}\n    \\mathbf{K}_n & \\mathbf{I}_n \\\\\n    \\mathbf{I}_n & \\mathbf{K}_n\n\\end{matrix}\\right]\\;.\n\\]\nHere, \\(\\mathbf{I}_n\\) is the \\(n\\times n\\) identity matrix.\nNow, \\(\\mathbf{A}_n\\) is the matrix of two cliques that have been “paired”, with each node in one clique connected to exactly one node in the other clique. A small example is shown in Figure 1.1.\n\n\nCode\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(tidyverse)\n\ncolors <- c(\"#f5b895\", \"#0f4c81\")\n\nn <- 5\n\nK <- create_complete(n)\n\nK <- K %>%\n    mutate(x = ifelse(row_number() <= 2, 0, 1),\n           y = ifelse(row_number() %% 2 == 0, 0, 1))\n\nG <- bind_graphs(K, K) %>%\n    mutate(id = row_number()) %>%\n    mutate(x = ifelse(id > n, x + 2, x),\n                 group = round((row_number() <= n)),\n                 group = factor(group)) \n\nG <- G %>%\n    bind_edges(tibble(from = 1:(n), to = 1:(n) + n))\n\nG %>% \n    ggraph() +\n    geom_edge_link(start_cap = circle(3, 'mm'),\n                   end_cap = circle(4, 'mm'), \n                   alpha = .3) +\n    geom_node_point(aes(fill = factor(group),  shape = factor(group)),  size = 10, color = \"black\") +\n    scale_shape_manual(values = c(21, 22)) + \n    scale_fill_manual(values = colors) +\n    guides(fill = \"none\", shape = \"none\") + \n    theme(aspect.ratio = 1,\n            panel.background = element_rect(fill = \"white\"),\n            plot.background = element_rect(fill = \"white\")) \n\n\n\n\n\n\nFigure 1.1: A visualization of the ‘paired cliques’ example with 10 total nodes. Nodes in the first clique are represented by orange circles, and nodes in the second clique by blue squares.\n\n\n\nWhat kinds of information are contained in the first few eigenvectors of \\(\\mathbf{A}_{2n}\\)?\n\n\n\n\n\n\nExercise: The vector of ones \\(\\mathbf{1}_{2n}\\) is an eigenvector of \\(\\mathbf{A}_{2n}\\). What is its eigenvalue? How do we know whether it is the largest one?\n\n\n\n\n\n\n\n\n\nExercise: The vector \\(\\mathbf{v} = (\\mathbf{1}_n, - \\mathbf{1}_n)\\) is another eigenvector of \\(\\mathbf{A}_{2n}\\). What is its eigenvalue?\n\n\n\nIn fact, these are the two largest eigenvalues of \\(\\mathbf{A}\\). The first one isn’t very interesting, but note that the second one actually separates the two cliques!\nSo, suppose we were given a graph where:\n\nWe knew that the graph had the paired-clique structure, but\nWe didn’t know which node belonged to which clique.\n\nA way to solve this problem would be to compute the second eigenvector \\(\\mathbf{v}\\). The signs of \\(\\mathbf{v}\\) separate the two cliques. This idea is the foundation of many spectral graph clustering algorithms.\nIn fact, the adjacency matrix isn’t usually the optimal matrix to use for spectral algorithms. This is a deep and important story related to random matrix theory, which has many connections to network science.\n\n\n(Nadakuditi and Newman 2012)\n\n\nDegrees\n\n\n\n\n\n\n\nDefinition 1.4 (Degree) \nThe degree \\(k_i\\) of a node \\(i\\) is the number of edges attached to it. \\[\nk_i = \\lvert \\left\\{j:(i,j) \\in E\\right\\}\\rvert\\;.\n\\]\n\n\n\n\nThe degree is a fundamental quantity in many network analyses. Especially the distribution of degrees in the network can play a major role in both theory and applications.\n\n\n\n\n\n\nExercise: show that the diagonal entries of \\(\\mathbf{A}^2\\) give the degree of each node.\n\n\n\nWe often collect the degrees into a diagonal matrix \\(\\mathbf{D}\\) whose diagonal entry \\(d_{ii} = k_i\\) contains the degree of node \\(i\\).\n\n\n\n\n\n\nExercise: Show that \\(\\sum_{i = 1}^n d_i = 2m\\), where \\(m\\) is the number of edges in \\(G\\).\n\n\n\n\n\nThe Laplacian\nAnother very important matrix for network representations is the graph Laplacian matrix. Actually, there are multiple matrices with claim to this name, but the one we’ll usually focus on is the combinatorial Laplacian.\n\n\n\n\n\n\n\nDefinition 1.5 (Combinatorial Graph Laplacian) \nThe combinatorial Laplacian \\(\\mathbf{L}\\) of a graph with adjacency matrix \\(\\mathbf{A}\\) is \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\).\n\n\n\n\n\n\n\n\n\n\nExercise: Given knowledge of the combinatorial Laplacian \\(\\mathbf{L}\\), is it possible to exactly reconstruct the graph?\n\n\n\nThe Laplacian is often used to represent (diffusive) flows of quantities between nodes. To see why, suppose that I have some amount of water \\(x_i\\) on each node \\(i\\), and that I collect this into a vector \\(\\mathbf{x}\\). Now, consider the vector \\(\\mathbf{L}\\mathbf{x}\\).\n\\[\n\\begin{align}\n    [\\mathbf{L}\\mathbf{x}]_i &= \\sum_{j} \\left(d_{ij}x_j - a_{ij}x_j \\right) \\\\\n    &= \\underbrace{k_ix_i}_{\\text{flow out of }i} - \\underbrace{\\sum_{j} a_{ij}x_j}_{\\text{flow into }i}\\;.\n\\end{align}\n\\]\nThe first term distributes the water \\(x_i\\) at node \\(i\\) to each of \\(i\\)’s \\(k_i\\) neighbors, while the second term allows water to flow into node \\(i\\) from each neighbor along each edge connecting them.\nThe Laplacian matrix \\(\\mathbf{L}\\) will appear in several places throughout this course, when we consider random walks, graph partitioning, and dynamical systems.\n\n\nMany More Matrices…\nThere are LOTS of matrices that can be associated to networks. There’s no “right” one – some are more useful than others for certain jobs. Throughout this course, we’ll see examples of matrices that are well-suited to certain specific tasks, like ranking or clustering. If you’re interested in searching around a bit, some other fun matrices are:\n\nThe nonbacktracking or Hashimoto matrix.\nThe modularity matrix.\nThe random-walk transition matrix.\nThe random-walk and symmetric normalized Laplacian matrices.\nThe PageRank matrix.\nThe node-edge incidence matrix.\n\nAnd the list goes on!\n\n\nDirected and Weighted Graphs\nNewman Chapter 6 contains a nice introductory discussion of directed and weighted graphs. We won’t spend a lot of time on these at this stage of the course, but it’s worthwhile reading this material as it may be of interest as you think about projects."
  },
  {
    "objectID": "chapters/measurement.html#measures-and-metrics",
    "href": "chapters/measurement.html#measures-and-metrics",
    "title": "1  Measuring Networks",
    "section": "Measures and Metrics",
    "text": "Measures and Metrics\nThere are lots of questions we can ask about network data. Even in the setting of simple graphs, there are some very interesting questions with surprisingly complex answers. In this section, we’ll consider some (not all) of the most important things to measure in networks.\n\nIdeas Connected to Walks\nWalks are a fundamental idea in networks. If we imagine networks as connecting things together, walks and walk counts are direct quantifications of how “connected” the network really is. Here are a few things we can measure about networks using walks.\n\nConnected Components\n\n\n\n\n\n\n\nDefinition 1.6 (Connected Components) \nTwo nodes \\(i\\) and \\(j\\) are path-connected if there exists a path \\(i \\leftrightarrow j\\). The set of nodes \\(j\\) such that \\(i\\) is path-connected to \\(j\\) is called the connected component of \\(i\\). A network is connected if it has only one connected component.\n\n\n\n\n\n\n\n\n\n\nExercise: Show that the relation “\\(i\\) is path-connected to \\(j\\)” is an equivalence relation on the set of nodes \\(N\\).\n\n\n\nIntuitively, a disconnected network is fragmented into two or more pieces. Almost all network analysis problems are most interesting and challenging when the network is connected, and so we’ll often assume this in data analysis applications. Mathematically, however, the question of when certain models of networks become connected has a very rich theory that we’ll see a small part of.\n\n\nGeodesics and Graph Diameter\n\n\n\n\n\n\n\nDefinition 1.7 (Geodesic Paths) \nThe length of a walk is the number of edges it contains. A geodesic path (also called a shortest path) from \\(i\\) to \\(j\\) is a walk fom \\(i\\) to \\(j\\) of minimum length; i.e. a walk such that no other walk has shorter length. The length of a geodesic path is called the (geodeisc) distance between \\(i\\) and \\(j\\). If two nodes are not path-connected, their geodesic distance is undefined.\n\n\n\n\n\n\nCode\nlibrary(tidygraph)\nlibrary(igraph)\nlibrary(ggraph)\n\nset.seed(1234)\n\ng <- erdos.renyi.game(20, 40, type = \"gnm\") %>%\n    as_tbl_graph()\n\npath <- shortest_paths(g, 5, 12)$vpath\n\nE(g, path=path[[1]])$on_path <- TRUE\n\ng <- g %>% \n    as_tbl_graph() %>% \n    activate(edges) %>% \n    mutate(on_path = !is.na(on_path))\n\ng %>% \n  ggraph() + \n  geom_edge_link(aes(width = on_path), alpha = 0.7) + \n  geom_node_point(size = 7, pch = 21, fill = \"#73b9ee\") + \n  theme_void() + \n  scale_edge_width_manual(values = c(.3, 2)) +  \n  guides(edge_color = \"none\", edge_width = \"none\")\n\n\n\n\n\nA graph with 20 nodes and 40 edges. While it can be difficult to navigate through even small graphs, note that some nodes are clearly close together, while others are far apart.  A geodesic (shortest path) is shown between two selected nodes. There is one node that is not connected to any of its neighbors, and its distance to them is undefined. As a result, this graph has undefined diameter.\n\n\n\n\n\n\n\n\n\n\nThe diameter of a graph is the longest length of geodesic paths across all pairs of nodes. If the graph is disconnected, the diameter is undefined.\n\n\n\nAnother way to say this is that, in a connected graph, the diameter is the distance between the two nodes that are farthest away.\n\n\nBetweenness Centrality\nHow important is a node? We’ll come back to this question several times in the course. The idea of geodesic paths allows us to consider one approach to this question:\n\n\n\n\n\n\nIdea: A node is important if you frequently have to go through it to get to other nodes.\n\n\n\nThis idea leads us to the idea of betweenness centrality.\n\n\n\n\n\n\n\nDefinition 1.8 (Betweenness Centrality) \nThe betweenness centrality \\(b_i\\) of a node \\(i\\) is the number of geodesic paths in the graph that pass through node \\(i\\).\n\n\n\n\nFigure 1.2 shows a network with nodes sized according to their betweenness centrality.\n\n\nCode\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(igraphdata)\ndata(karate)\n\ng <- karate %>% \n    as_tbl_graph() %>% \n    mutate(btwn = centrality_betweenness())\n\ng %>% \n    ggraph() +\n    geom_edge_link(alpha = 0.2) + \n    geom_node_point(aes(size = btwn), pch = 21, fill = \"#73b9ee\") + \n    theme_void() + \n    guides(size = \"none\") + \n    scale_size_continuous(range = c(3, 15))\n\n\n\n\n\nFigure 1.2: The famous Zachary Karate Club network (Zachary 1977). The size of each node reflects its betweenness centrality.\n\n\n\n\n\n\nKatz Centrality\nHere’s another centrality concept that is going to draw on our linear algebra skills a bit.\nRecall that \\(\\mathbf{A}^k\\) has entries that count the total number of walks of length \\(k\\) between two nodes. Here’s an alternative idea of node importance:\n\n\n\n\n\n\nA node \\(i\\) is important when there are lots of short walks in the graph that lead to \\(i\\).\n\n\n\nThis idea takes a bit more effort to operationalize mathematically.\nFirst, the number of walks of length \\(k\\) that lead to each node can be computed by the vector \\(\\mathbf{v}(k) = \\mathbf{A}^k\\mathbf{1}_n\\). Now, \\(v_i(k)\\) gives the total number of \\(k\\)-walks that lead to node \\(i\\). Now, the total number of walks of length \\(\\ell\\) or less leading to \\(i\\) is \\(\\sum_{k = 1}^{\\ell} v_i(k)\\).\nNow, if we all \\(\\ell \\rightarrow \\infty\\), then we’ll find that the total number of walks to \\(i\\) is infinite. But we can make progress if we instead decide that short walks are more important than longer walks. This makes sense – if I have to go on a long walk to find you, then you might be in a relatively remote, out-of-the-way location.\nSo, we’ll add a discount factor \\(\\alpha < 1\\). A walk of length \\(k\\) gets a discount of \\(\\alpha^k\\). The weighted number of walks to \\(i\\) is \\(\\sum_{k = 1}^{\\infty} \\alpha^kv_i(k)\\). The vector containing this information for each node is the vector\n\\[\n\\mathbf{v}(\\alpha) = \\sum_{k = 1}^{\\infty} \\alpha^k\\mathbf{v}(k)\\;.\n\\]\nPerhaps surprisingly, we’re not stuck. Recall the definition of \\(\\mathbf{v}(k)\\), which lets us write\n\\[\n\\mathbf{v}(\\alpha) = \\sum_{k = 1}^{\\infty} \\alpha^k\\mathbf{A}^k\\mathbf{1}_n = \\left[\\sum_{k = 1}^{\\infty} (\\alpha\\mathbf{A})^k\\right]\\mathbf{1}_n\\;.\n\\]\nNow we need a matrix trick.\n\n\n\n\n\n\nIf \\(\\mathbf{B}\\in \\mathbb{R}^{n\\times n}\\) is a symmetric matrix and \\(\\left|\\lambda\\right| < 1\\) for any eigenvalue \\(\\lambda\\) of \\(\\mathbf{B}\\), then\n\\[\n\\sum_{k = 1}^{\\infty} \\mathbf{B}^k = (\\mathbf{I}_n - \\mathbf{B})^{-1} - \\mathbf{I}_n\\;.\n\\]\n\n\n\n\n\nCompare this theorem to the geometric series formula \\[\n\\sum_{k = 1}^{\\infty} r^k = (1 - r)^{-1} - 1\n\\] for scalar \\(\\left|r \\right| < 1\\).\nWe can use this result to compute \\(\\mathbf{v}(\\alpha)\\). Provided that \\(\\alpha\\) is smaller than \\(\\frac{1}{\\left|\\lambda\\right|}\\) for any eigenvalue \\(\\lambda\\) of \\(\\mathbf{A}\\), we have the Katz centrality.\n\n\n\n\n\n\n\nDefinition 1.9 \nThe Katz centrality (Katz 1953) of node \\(i\\) with parameter \\(\\alpha\\) is the \\(i\\)th entry of the vector \\[\n\\mathbf{v}(\\alpha) = ((\\mathbf{I}_n - \\alpha\\mathbf{A})^{-1}  - \\mathbf{I}_n)\\mathbf{1}_n\\;.\n\\]\n\n\n\n\n\n\nCode\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(igraphdata)\ndata(karate)\n\ng <- karate %>% \n    as_tbl_graph() %>% \n    mutate(btwn = centrality_katz(alpha = .05))\n\ng %>% \n    ggraph() +\n    geom_edge_link(alpha = 0.2) + \n    geom_node_point(aes(size = btwn), pch = 21, fill = \"#73b9ee\") + \n    theme_void() + \n    guides(size = \"none\") + \n    scale_size_continuous(range = c(3, 15))\n\n\n\n\n\nFigure 1.3: The Zachary Karate Club network (Zachary 1977). The size of each node reflects its Katz centrality with α = 0.05.\n\n\n\n\nAs shown in Figure 1.3, the Katz centrality leads to less dramatic results, while still highlighting nodes that appear to be “in the center” of the graph.\n\n\n\nTriadic Closure (Transitivity)\nThink of two of your friends. Are they friends with each other? This kind of “coincidence” happens quite frequently in social networks. Let’s look at two ways to quantify this effect.\nIn graph terms, when you think of two of your friends, you are thinking of two edges connected to you in your social network, and asking whether there exists a third edge that creates a triangle. So, we have a situation like that shown in Figure 1.4.\n\n\nCode\nlibrary(ggraph)\nlibrary(tidyverse)\nlibrary(tidygraph)\n\ntbl_graph(\n    edges = tibble(\n        from = c(1, 1, 3), \n        to = c(2, 3, 2),\n        q = c(FALSE, FALSE, TRUE)\n        )\n    ) %>% \n    ggraph() + \n    geom_edge_link(aes(edge_linetype = q), alpha = 0.6, edge_width = 3) + \n    geom_node_point(pch = 21, fill = \"#73b9ee\", size = 15) + \n    theme_void() + \n    guides(edge_linetype = \"none\")\n\n\n\n\n\nFigure 1.4: Triadic closure. The two solid edge form a wedge (a possible triangle). The dashed edge turns the wedge into a (realized) triangle.\n\n\n\n\nWhen the dotted line “closes the triangle,” we often call this “closing the triangle.* The phrase triadic closure, also sometimes called transitivity, refers to the existence of large numbers of closed triangles in many social networks.\nHow do we quantify triadic closure? There’s a simple approach to this:\n\n\n\n\n\n\n\nDefinition 1.10 (Local Clustering) The local clustering coefficient \\(C_i\\) measures the proportion of possible triangles connected to \\(i\\) that are realized. If \\(i \\sim j\\) means “\\(i\\) and \\(j\\) are connected, then we can write this as\n\\[\nC_i = \\frac{\\sum_{j, \\ell \\sim i} a_{j\\ell}}{\\binom{k_i}{2}}\n\\]\nRemember: \\(k_i\\) is the degree (number of neighbors) of node \\(i\\).\n\n\n\n\nIn this definition, the denominator is the number of pairs of neighbors of node \\(i\\). We can think of this as counting the number of possible triangles. The numerator is then the number of realized triangles. The local clustering coefficient is always a number between 0 and 1.\n\n\n\n\n\n\nExercise: Write the numerator in terms of the adjacency matrix \\(\\mathbf{A}\\).\nHint: a triangle is a special kind of walk of length \\(3\\).\n\n\n\nThe local clustering coefficient tells us about the rate of triadic closure around a specific node \\(i\\). There is also a global clustering coefficient that quantifies the rate of triadic closure in the entire network. The idea is the same: it’s the proportion of possible triangles that are actualized:\n\n\n\n\n\n\n\nDefinition 1.11 (Global Clustering) \nThe global clustering coefficient of a graph is \\[\n\\frac{\\text{Number of triangles} \\times 3}{\\text{Number of wedges}}\n\\] Here, a wedge is a possible triangle, like the solid lines shown in the figure above.\n\n\n\n\nThe reason for the factor of 3 in the numerator is that the same triangle can be formed from 3 different wedges.\n\n\ncf. Newman p. 184\n\n\n\n\n\n\nIt is not the case that the global clustering coefficient can be obtained by averaging the local clustering coefficients!\n\n\n\nThe prevalence of triadic closure in many real-world networks motivated one of the first mega-papers in network science, which proposed a mathematical model of networks that included clustering (Watts and Strogatz 1998). A sample from this model is shown in Figure 1.5. This paper has now been cited 48,000 times over 24 years.\n\n\nCode\nlibrary(igraph)\nlibrary(ggraph)\n\ng <- sample_smallworld(1, 20, 3, 0.05)\n\ng %>% \n    ggraph() +\n    geom_edge_link(alpha = 0.2) + \n    geom_node_point(pch = 21, fill = \"#73b9ee\", size = 6) + \n    theme_void() + \n    guides(size = \"none\")\n\n\n\n\n\nFigure 1.5: A small network sampled from the small-world model of Watts and Strogatz (1998). This model generates graphs with high clustering coefficients. You can likely pick out a large number of triangles by eye.\n\n\n\n\n\n\nCommunity Structure and Graph Cuts\nMany networks display what is sometimes called community structure. There isn’t really a formal definition of community structure, but it refers broadly to the idea that a graph can often be divided into multiple parts. Here’s an example of a graph with community structure:\n\n\nCode\n# R code\nlibrary(tidygraph)\nlibrary(ggraph)\n\nset.seed(1234)\n\ng <- play_islands(2, 10, 0.7, 2)\n\ng %>% \n  ggraph() + \n  geom_edge_link(alpha = 0.5) + \n  geom_node_point(size = 7, pch = 21, fill = \"#73b9ee\", color = \"black\") + \n  theme_void()\n\n\n\n\n\nA network with 20 nodes. This network displays one of many common properties in real-world networks, called community structure. In this case, the network is clustered into two densely interconnected ‘communities’ that are only loosely connected to each other. Community structure often accompanies phenomena such as echo chambers in online media.\n\n\n\n\nYou might be able to visually separate this graph into two pieces. Heuristically, what we’re looking for is lots of edges within each piece, but few edges between the pieces.\nLet’s formalize this a bit.\n\n\n\n\n\n\n\nDefinition 1.12 (Clustering) A clustering \\((C_1,\\ldots,C_\\ell)\\) of a graph is a partition of the nodes:\n\\[\nN = \\bigcup_{j = 1}^{\\ell} C_j\\;, \\quad C_j \\cap C_{j'} = \\emptyset\\;.\n\\]\nWe’ll write the cluster holding node \\(i\\) as \\(c_i\\).\n\n\n\n\nNote: “clustering” here is confusingly and totally unrelated to the “clustering coefficients” from before.\n\n\n\n\n\n\n\nDefinition 1.13 (Cut Size) The cut size of a clustering is the number of edges that connect nodes in different clusters:\n\\[\n\\mathrm{Cut}(C_1,\\ldots,C_\\ell) = \\frac{1}{2}\\sum_{i \\neq j} a_{ij}\\mathbb{1}[c_i \\neq c_j]\n\\]\n\n\n\n\nThe reason for the factor \\(1/2\\) is that we don’t need to count the edges \\((i,j)\\) and \\((j,i)\\) separately – it’s the same edge!\nAn important special case arises when we aim to partition a graph into two clusters. In this case, we can write the cut size in terms of our friend, the combinatorial graph Laplacian.\nLet \\(\\mathbf{s} \\in \\mathbb{R}^n\\) be the vector with entries\n\\[\ns_i = \\begin{cases}+1 &\\quad i \\in C_1 \\\\ -1 &\\quad i \\in C_2\\end{cases}\\;.\n\\]\n\n\n\n\n\n\n\nTheorem 1.1 (Laplacian Formula for Cuts) \nWe have \\(\\mathrm{Cut}(C_1,C_2) = \\frac{1}{4}\\mathbf{s}^T\\mathbf{L}\\mathbf{s}\\).\n\n\n\n\n\n\n\n\n\n\n\nProof. To prove this, let’s start by just computing \\(\\mathbf{x}^T\\mathbf{L}\\mathbf{x}\\) for arbitrary \\(\\mathbf{x} \\in \\mathbb{R}^n\\). We need to recall the definition of \\(\\mathbf{L}\\) from Definition 1.5. We have \\[\n\\begin{aligned}\n\\mathbf{x}^T\\mathbf{L}\\mathbf{x} &= \\mathbf{x}^T(\\mathbf{D} - \\mathbf{A})\\mathbf{x} \\\\\n&= \\mathbf{x}^T\\mathbf{D}\\mathbf{x}^T - \\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\\\\n&= \\sum_{i\\in N} k_{i} x_i^2 - \\sum_{i,j \\in N} a_{ij}x_ix_j \\\\\n&= \\frac{1}{2}\\left(\\sum_{i\\in N} k_{i} x_i^2 - 2\\sum_{i,j \\in N} a_{ij}x_ix_j - \\sum_{j\\in N} k_{j} x_j^2\\right) \\\\\n&= \\frac{1}{2}\\left(\\sum_{i,j\\in N} a_{ij} x_i^2 - 2\\sum_{i,j \\in N} a_{ij}x_ix_j - \\sum_{i,j\\in N} a_{ij} x_j^2\\right) \\\\\n&= \\frac{1}{2}\\sum_{i,j \\in N}a_{ij}(x_i - x_j)^2\\;.\n\\end{aligned}\n\\]\nLet’s use this formula to evaluate \\(\\mathbf{s}^T\\mathbf{L}\\mathbf{s}\\):\n\\[\n\\begin{aligned}\n\\mathbf{s}^T\\mathbf{L}\\mathbf{s} &= \\sum_{i,j \\in N}a_{ij}(s_i - s_j)^2 \\\\\n&= \\sum_{i,j \\in N}a_{ij}(s_i - s_j)^2 \\\\\n&= 4 \\sum_{i,j \\in N}a_{ij}\\mathbb{1}[c_i \\neq c_j]\\;.\n\\end{aligned}\n\\]\nIn that last line, we’ve used the fact that \\[\n(s_i - s_j)^2 = \\begin{cases}\n    2^2 &\\quad c_i \\neq c_j \\\\\n    0 &\\quad \\text{otherwise}\n\\end{cases} \\quad = \\mathbb{1}[c_i \\neq c_j]\\;.\n\\]\nSo, we’re done!\n\n\n\n\n\n\nLooking Ahead\nAn important question to ask about many of these metrics is:\n\nWhat counts as a large, surprising, or meaningful value of a given metric?\n\nOne way to address this question is to take some value and compare it to one that we believe to be small, unsurprising, or not meaningful. We often operationalize this by saying that “a random graph” would have some property, such as a low clustering coefficient. So, a high clustering coefficient is surprising, and suggests that the graph might not be “random.” This motivation takes directly to the study of random graphs, which is one of the mathematical foundations of network science."
  },
  {
    "objectID": "chapters/random_graphs.html",
    "href": "chapters/random_graphs.html",
    "title": "2  Random Graphs: Erdős–Rényi",
    "section": "",
    "text": "A random graph is a probability distribution over graphs. In this set of lecture notes, we’ll begin our exploration of several random graphs."
  },
  {
    "objectID": "chapters/random_graphs.html#why-random-graphs",
    "href": "chapters/random_graphs.html#why-random-graphs",
    "title": "2  Random Graphs: Erdős–Rényi",
    "section": "2.1 Why Random Graphs?",
    "text": "2.1 Why Random Graphs?\nWhy should we even study random graphs? There are a few reasons!\n\nRandom Graphs as Insightful Models\nRandom graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations.\n\n\nRandom Graphs as Mathematical Puzzles\nMany properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices.\n\n\nRandom Graphs as Null Hypotheses\nSuppose that you compute the global clustering coefficient (Definition 1.11) of a graph and find it to be \\(0.31\\). How do we interpret that? Is that high? Low? In this case, we should ask: compared to what? Random graphs allow us one way to make a comparison: that clustering coefficient is “high” if it’s larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear!\nIn this way, random graphs often serve as the null hypothesis, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests.\n\n\nRandom Graphs and Statistical Inference\nMany statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference, and we’ll see a few examples later in the course."
  },
  {
    "objectID": "chapters/random_graphs.html#the-erdősrényi-model",
    "href": "chapters/random_graphs.html#the-erdősrényi-model",
    "title": "2  Random Graphs: Erdős–Rényi",
    "section": "2.2 The Erdős–Rényi Model",
    "text": "2.2 The Erdős–Rényi Model\nThe model of Erdős and Rényi (1960) is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is mostly understood in its major mathematical properties, and it’s almost never used in statistical inference. The ER model is, however, a building block of models that are used in statistical inference. The stochastic blockmodel, for example, is often used for graph clustering and community detection. In its simplest form, it’s a bunch of ER graphs glued together.\n\n\n\n\n\n\n\nDefinition 2.1 (Erdős–Rényi Random Graph) \nAn Erdős–Rényi random graph with \\(n\\) nodes and connection probability \\(p\\), written \\(G(n,p)\\), is a random graph constructed by placing an edge with probability \\(p\\) between each pair of distinct nodes.\n\n\n\n\nWe can imagine visiting each possible pair of edges \\((i,j)\\) and flipping a coin with probability of heads \\(p\\). If heads, we add \\((i,j) \\in E\\); otherwise, we don’t.\n\n\n\n\n\n\nExercise: Let \\(i\\) be a fixed node in a \\(G(n,p)\\) graph, and let \\(K_i\\) be its (random) degree. Show that \\(K_i\\) has binomial distribution with success probability \\(p\\) and \\(n-1\\) trials.\n\n\n\n\n\n\n\n\n\nExercise: Show that \\(\\mathbb{E}[K_i] = p(n-1)\\).\n\n\n\n\nClustering Coefficient\nRecall that both local and global clustering coefficients were defined in terms of a ratio of realized triangles to possible triangles. Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately.\nLet’s take the global clustering coefficient (Definition 1.11). For this, we need to compute the total number of triangles, and the total number of wedges.\nHow many triangles are there? Well, there are \\(\\binom{n}{3}\\) ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is \\(p^3\\). So, in expectation, there are \\(\\binom{n}{3}p^3\\) triangles in the graph.\nHow many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is \\(p^2\\). So, the expected number of wedges is \\(\\binom{n}{3}p^2\\). Our estimate for the expected clustering coefficient is that it should be about \\(p\\), although we have been fast and loose with several mathematical details to arrive at this conclusion."
  },
  {
    "objectID": "chapters/random_graphs.html#sparsity",
    "href": "chapters/random_graphs.html#sparsity",
    "title": "2  Random Graphs: Erdős–Rényi",
    "section": "2.3 Sparsity",
    "text": "2.3 Sparsity\nWe are very often interested in the sparse Erdős–Rényi model. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes.\n\n\n\n\n\n\n\nDefinition 2.2 \nWe say that a \\(G(n,p)\\) graph is sparse when \\(p = c/(n-1)\\) for some constant \\(c\\).\n\n\n\n\nA consequence of sparsity is that \\(\\mathbb{E}[K_i] = c\\); i.e. the expected degree of a node in sparse \\(G(n,p)\\) is constant. When studying sparse \\(G(n,p)\\), we are almost always interested in the case \\(n\\rightarrow \\infty\\).\n\nClustering\nWhat does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be about \\(p\\), and if \\(p = c/(n-1)\\), then \\(p \\rightarrow 0\\).\nThe need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of Watts and Strogatz (1998), a famous paper that introduced the “small world model.”\n\nSparse Erdős–Rényi graphs have vanishing clustering coefficients.\n\nFigure 2.1 shows how the global clustering coefficient of a sparse Erdős–Rényi random graph decays as we increase \\(n\\). Although the estimate that the global clustering coefficient should be equal to \\(p\\) was somewhat informal, experimentally it works quite well.\n\n\nCode\nimport networkx as nx\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nc = 5\nN = np.repeat(2**np.arange(5, 15), 10)\n\ndef global_clustering(n, c):\n    G = nx.fast_gnp_random_graph(n, c/(n-1))\n    return nx.transitivity(G)\n\nT = [global_clustering(n, c) for n in N]\n\nfig, ax = plt.subplots(1)\n\ntheory = ax.plot(N, c/(N-1), color = \"black\", label = \"Estimate\")\n\nexp = ax.scatter(N, T, label = \"Experiments\")\nsemil = ax.semilogx()\nlabels = ax.set(xlabel = \"Number of nodes\", \n       ylabel = \"Global clustering coefficient\")\nplt.show()\n\n\n\n\n\nFigure 2.1: Each point gives the global clustering coefficient of an ER graph with mean degree \\(c = 5\\) and specified number of nodes. The black line gives the estimate \\(T = p = c/(n-1)\\) for the clustering coefficient that we computed earlier. The function nx.fast_gnp_random_graph() is a special function from NetworkX for generating ER random graphs quickly; you’ll learn about how it works in an upcoming homework assignment.\n\n\n\n\n\n\nCycles and Local Tree-Likeness\n\n\n\n\n\n\n\nDefinition 2.3 \nA cycle is a walk that does not repeat edges and ends at the same node that it begins.\n\n\n\n\nA triangle is an example of a cycle of length \\(3\\).\n\n\n\n\n\n\n\nTheorem 2.1 \nIn the sparse \\(G(n,p)\\) model, for any length \\(k\\), the probability that there exists a cycle of length \\(k\\) attached to node \\(i\\) shrinks to 0 as \\(n \\rightarrow \\infty\\).\n\n\n\n\nYou’ll prove a generalization of Theorem 2.1 in an upcoming homework problem.\nGraphs in which cycles are very rare are often called locally tree-like. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble.\n\n\nPath Lengths\nHow far apart are two nodes in \\(G(n,p)\\)? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail.  However, we can get a big-picture view of the situation by asking a slightly different question:See Riordan and Wormald (2010) and references therein.\n\nGiven two nodes \\(i\\) and \\(j\\), what is the expected number of paths of length \\(k\\) between them?\n\nCall the number of \\(k\\)-paths between \\(i\\) and \\(j\\) \\(R(k)\\). Let \\(r(k) = \\mathbb{E}[R(k)]\\). Let’s estimate \\(r(k)\\).\nFirst, we know that \\(r(1) = p\\). For higher values of \\(k\\), we’ll use the following idea: in order for there to be a path of length \\(k\\) from \\(i\\) to \\(j\\), there must be a node \\(\\ell\\) such that:\n\nThere exists a path from \\(i\\) to \\(\\ell\\) of length \\(k-1\\). In expectation, there are \\(r(k-1)\\) of these.\nThere exists a path from \\(\\ell\\) to \\(j\\) of length \\(1\\). This happens with probability \\(p\\).\n\nThere are \\(n-2\\) possibilities for \\(\\ell\\) (excluding \\(i\\) and \\(j\\)), and so we obtain the approximate relation\n\\[\nr(k) \\approx (n-2)p r(k-1)\\;.\n\\]\n\n\n\n\n\n\nWhy is this an approximation? Well, some of the paths between \\(i\\) and \\(\\ell\\) that are counted in \\(r(k-1)\\) could actually include the edge \\((j, \\ell)\\) already. An example is \\((i,j), (j,\\ell)\\). In this case, the presence of edge \\((j,\\ell)\\) is not independent of the presence of the path between \\(i\\) and \\(\\ell\\). The derivation above implicitly treats these two events as independent. Again, because cycles are rare in large, sparse ER, this effect is small when \\(k\\) is small.\n\n\n\nProceeding inductively and approximating \\(n-2 \\approx n-1\\) for \\(n\\) large, we have the relation\n\\[\nr(k) \\approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p\n\\qquad(2.1)\\]\nin the sparse ER model.\nUsing this result, let’s ask a new question:\n\nWhat path length \\(k\\) do I need to allow to be confident that there’s a path between nodes \\(i\\) and \\(j\\)?\n\nWell, suppose we want there to be \\(q\\) paths. Then, we can solve \\(q = c^{k-1}p\\) for \\(k\\), which gives us:\n\\[\n\\begin{aligned}\nq &= c^{k-1}p \\\\\n\\log q &= (k-1)\\log c + \\log p \\\\\n\\log q &= (k-1)\\log c + \\log c - \\log n \\\\\n\\frac{\\log q + \\log n}{\\log c} &= k\n\\end{aligned}\n\\]\nHere, I’ve also approximated \\(\\log n-1 \\approx \\log n\\) for \\(n\\) large.\nSo, supposing that I want there to be at least one path in expectation (\\(q = 1\\)), I need to allow \\(k = \\frac{\\log n}{\\log c}\\). This is pretty short, actually! For example, the population of the world is about \\(8\\times 10^9\\), and Newman estimates that an average individual knows around 1,000 other people; that is, \\(c = 10^3\\) in the world social network. This seems a little high to me (maybe I’m antisocial), but the resulting value of \\(k\\) here is around 3.3.\nIn other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than \\(4\\).\nMore formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of \\(n\\), even in relatively sparse cases.\n\n\nA Caveat\nIf you spend some time looking at Equation 2.1, you might find yourself wondering:\n\nHey, what happens if \\(c \\leq 1\\)?\n\nIndeed, something very interesting happens here. Let’s assume \\(c < 1\\) (i.e. we’re ignoring the case \\(c = 1\\)), and estimate the expected number of paths between \\(i\\) and \\(j\\) of any length. Using Equation 2.1, we get\n\\[\n\\mathbb{E}\\left[\\sum_{k = 1}^{\\infty} R(k)\\right] = \\sum_{k = 1}^\\infty c^{k-1}p = \\sum_{k = 0}^\\infty c^kp = \\frac{p}{1-c}\\;.\n\\]\nIf we now use Markov’s inequality, we find that the probability that there is a path of any length between nodes \\(i\\) and \\(j\\) is no larger than\n\\[\n\\frac{p}{1-c} = \\frac{c}{(1-c)(n-1)}\\rightarrow 0\\;.\n\\]\nSo, this suggests that, if \\(c < 1\\), any two nodes are likely to be disconnected! On the other hand, if \\(c > 1\\), we’ve argued that we can make \\(k\\) large enough to have high probability of a path of length \\(k\\) between those nodes.\nSo, what’s special about \\(c = 1\\)? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let’s study in a bit more detail the sizes of the connected components of the ER graph."
  },
  {
    "objectID": "chapters/random_graphs.html#component-sizes-and-the-branching-process-approximation",
    "href": "chapters/random_graphs.html#component-sizes-and-the-branching-process-approximation",
    "title": "2  Random Graphs: Erdős–Rényi",
    "section": "2.4 Component Sizes and the Branching Process Approximation",
    "text": "2.4 Component Sizes and the Branching Process Approximation\nWe’re now going to ask ourselves about the size of a “typical” component in the Erdős–Rényi model. In particular, we’re going to be interested in whether there exists a component that fills up “most” of the graph, or whether components tend to be vanishingly small in relation to the overall graph size.\nOur first tool for thinking about this question is the branching process approximation. Informally, a branching process is a process of random generational growth. We’ll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram:\n\n\n\n\n\nImage source.\n\n\n\n\nWe start with a single entity, \\(X_0\\). Then, \\(X_0\\) has a random number of “offspring”: \\(X_1\\) in total. Then, each of those \\(X_1\\) offspring has some offspring of their own; the total number of these offspring is \\(X_2\\). The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process “dies out.”\n\n\nSome of this exposition in this section draws on these notes by David Aldous.\n\n\n\n\n\n\n\nDefinition 2.4 (Branching Process) Let \\(p\\) be a probability distribution on \\(\\mathbb{Z}\\), called the offspring distribution.\nA branching process with distribution \\(p\\) is a sequence of random variables \\(X_0, X_1,,X_2\\ldots\\) such that \\(X_0 = 1\\) and, for \\(t \\geq 1\\),\n\\[\nX_t = \\sum_{i = 1}^{X_{t-1}} Y_i\\;,\n\\]\nwhere each \\(Y_i\\) is distributed i.i.d. according to \\(p\\).\n\n\n\n\n\n\nTechnically, this is a Galton-Watson branching process, named after the two authors who first proposed it (Watson and Galton 1875).   History note: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames.\n\nApplication to Erdős–Rényi\nBranching processes create trees – graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that cycles are rare in Erdős–Rényi random graphs. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well.\nHere’s the particular form of the branching process approximation that we will use:\n\n\n\n\n\n\n\nDefinition 2.5 (Branching Process Approximation for ER Component Sizes) Sample a single node \\(j\\) at random from a large, sparse ER graph with mean degree \\(c\\), and let \\(S\\) be the size (number of nodes) of the component in which \\(j\\) lies. Note that \\(S\\) is random: it depends both on \\(j\\) and on the realization of the ER graph.\nThen, \\(S\\) is distributed approximately as \\(T\\), where \\(T = \\sum_{i = 0}^{\\infty}X_t\\) is the total number of offspring in a GW branching process with offspring distribution \\(\\text{Poisson}(c)\\).\n\n\n\n\nThe idea behind this approximation is:\n\nWe start at \\(j\\), which has \\(\\text{Poisson}(c)\\) (as you proved in HW).\nEach of these neighbors also has \\(\\text{Poisson}(c)\\) new neighbors, and so on.\nWe keep visiting new neighbors until we run out, and add up the number of neighbors we’ve visited to obtain \\(S\\).\n\nSince cycles are rare in ER, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process also approximately describes \\(T\\) in a branching process with a \\(\\text{Poisson}(c)\\) offpsring distribution.\n\n\n\n\n\n\n\nExercise: In the second generation, we get to a new node by following an edge. For that reason, shouldn’t the number of new edges be \\(\\text{Poisson}(c-1)\\) rather than \\(\\text{Poisson}(c-1)\\)? Why or why not?\n\n\n\n\n\nThe Subcritical Case\nThe mean of a \\(\\text{Poisson}(c)\\) random variable is again \\(c\\). As you’ll show in homework, this implies that \\(X_t\\), the number of offspring in generation \\(t\\), satisfies \\(\\mathbb{E}[X_t] = c^{t}\\). It follows that, when \\(c < 1\\), \\(\\mathbb{E}[T] = \\frac{1}{1-c}\\).\nNow using Markov’s inequality, we obtain the following results:\n\n\n\n\n\n\nIn a \\(\\text{Poisson}(c)\\) branching process with \\(c < 1\\), \\[\\mathbb{P}(X_t > 0) \\leq c^t\\;.\\]\n\n\n\nSo, the probability that the branching process hasn’t yet “died out” decays exponentially with timestep \\(t\\). In other words, the branching process becomes very likely to die out very quickly.\n\n\n\n\n\n\nIn a \\(\\text{Poisson}(c)\\) branching process with \\(c < 1\\), \\[\\mathbb{P}(T > a) \\leq \\frac{1}{a}\\frac{1}{1-c}\\]\n\n\n\nIn particular, for \\(a\\) very large, we are guaranteed that \\(\\mathbb{P}(T > a)\\) is very small.\nSumming up, when \\(c < 1\\), the GW branching process dies out quickly and contains a relatively small number of nodes: \\(\\frac{1}{1-c}\\) in expectation. In this setting, the branching process is called subcritical.\n\nBack to ER\nIf we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic:\n\n\n\n\n\n\nHeuristic: In a sparse ER random graph with mean \\(c < 1\\), the expected size of a component containing a randomly selected node is roughly \\(\\frac{1}{1-c}\\).\nIn particular, since this quantity is independent of \\(n\\), we find that the fraction of the graph occupied by this component is \\(\\frac{1}{n}\\frac{1}{1-c}\\) and therefore vanishes as \\(n\\rightarrow \\infty\\).\nWe can also turn this into a statement about probabilities: Markov’s inequality implies that, if \\(S\\) is the size of a component containing a randomly selected node,\n\\[\n\\mathbb{P}(S/n > a) \\rightarrow 0\n\\]\nfor any constant \\(a > 0\\). In other words, for large \\(n\\), the largest component is always vanishingly small in relation to the graph as a whole.\n\n\n\nLet’s check this experimentally. The following code block computes the size of the component in an ER graph containing a random node, and averages the result across many realizations. The experimental result is quite close to the theoretical prediction.\n\n\nCode\nimport networkx as nx\nimport numpy as np\n\ndef component_size_of_node(n, c):\n    G = nx.fast_gnp_random_graph(n, c/(n-1))\n    return len(nx.node_connected_component(G, 1))\n\nc = 0.8\nsizes = [component_size_of_node(5000, c) for i in range(1000)]\n\nout = f\"\"\"\nAverage over experiments is {np.mean(sizes):.2f}.\\n\nTheoretical expectation is {1/(1-c):.2f}.\n\"\"\"\n\nprint(out)\n\n\n\nAverage over experiments is 5.45.\n\nTheoretical expectation is 5.00.\n\n\nNote that the expected (and realized) component size is very small, even though the graph contains 5,000 nodes!\nFor this reason, we say that subcritical ER contains only small connected components, in the sense that each component contains approximately 0% of the graph as \\(n\\) grows large.\nThis explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are on the same connected component. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes.\n\n\n\nThe Giant Component\n\n\n\n\n\n\n\nDefinition 2.6 (Giant Component) We say that \\(G(n,p)\\) has a giant component if \\[\n\\mathbb{P}(S/n > a) \\rightarrow b\n\\] for some constant \\(b > 0\\).\nIntuitively, this means that there is a possibility of a connected component that takes up a nonzero fraction of the graph.\n\n\n\n\nSo far, we’ve argued using the branching process approximation that there is no giant component in the Erdős–Rényi model with \\(c < 1\\). The theory of branching processes also suggests to us that there could be a giant component when \\(c > 1\\).\nThe proof of this fact is usually done in terms of generating functions and is beyond our scope, but you can check Wikipedia for an outline.\n\n\n\n\n\n\nFact: when \\(c > 1\\), there is a nonzero probability that the \\(\\text{Poisson}(c)\\) branching process continues forever; that is, never goes extinct.\n\n\n\nUsing our correspondence between components of the ER model and branching processes, this suggests that, if we pick a random node, the component it is in has the potential to be very large. In fact (and this requires some advanced probability to prove formally), when \\(c > 1\\), there is a giant component. This is our first example of a phase transition. It is also possible to prove that, with high probability, there is only one giant component; Newman does this in 11.5.1.\n\n\n\n\n\n\n\nDefinition 2.7 (Phase Transition) \nA phase transition is a qualitative change in response to a small variation in a quantitative parameter.\n\n\n\n\nExamples of phase transitions include freezing, in which a liquid undergoes a qualitative change into a solid in response to a small variation in temperature.\nFigure 2.2 shows two sparse ER random graphs on either side of the \\(c = 1\\) transition. We observe an apparent change in qualitative behavior between the two cases.\n\n\nCode\nimport networkx as nx\nfrom matplotlib import pyplot as plt\n\nfig, axarr = plt.subplots(1, 2)\n\nn = 500\nc = [0.7, 1.3]\n\nfor i in range(2):\n    G = nx.fast_gnp_random_graph(n, c[i]/(n-1))\n    nx.draw(G, ax = axarr[i], node_size = 1)\n    axarr[i].set(title = f\"c = {c[i]}\")\n\n\n\n\n\nFigure 2.2: Two sparse ER graphs with 500 nodes and varying mean degree.\n\n\n\n\n\nSize of the Giant Component\nPerhaps surprisingly, while it’s difficult to prove that there is a giant component, it’s not hard at all to estimate its size. This argument is reproduced from Newman, pages 349-350\nLet \\(S\\) be the size of the giant component in an Erdős–Rényi random graph, assuming there is one. Then, \\(s = S/n\\) is the probability that a randomly selected node is in the giant component. Let \\(u = 1 - s\\) be the probability that a given node is not in the giant component.\nLet’s take a random node \\(i\\), and ask it the probability that it’s in the giant component. Well, one answer to that question is just “\\(u\\).” On the other hand, we can also answer that question by looking at \\(i\\)’s neighbors. If \\(i\\) is not in the giant component, then it can’t be connected to any node that is in the giant component. So, for each other node \\(j\\neq i\\), it must be the case that either:\n\n\\(i\\) is not connected to \\(j\\). This happens with probability \\(1-p\\).\n\\(i\\) is connected to \\(j\\), but \\(j\\) is not in the giant component either. \\(i\\) is connected to \\(j\\) with probability \\(p\\), and \\(j\\) is not in the giant component with probability \\(u\\).\n\nThere are \\(n-1\\) nodes other than \\(i\\), and so the probability that \\(i\\) is not connected to any other node in the giant component is \\((1 - p + pu)^{n-1}\\). We therefore have the equation\n\\[\nu = (1 - p + pu)^{n-1}\\;.\n\\]\nLet’s take the righthand side and use \\(p = c/(n-1)\\): \\[\n\\begin{aligned}\n    u &= (1 - p(1-u))^{n-1} \\\\\n      &= \\left(1 - \\frac{c(1-u)}{n-1}\\right)^{n-1}\\;.\n\\end{aligned}\n\\] This is a good time to go back to precalculus and remember the limit definition of the function \\(e^x\\): \\[\ne^x = \\lim_{n \\rightarrow \\infty}\\left(1 + \\frac{x}{n}\\right)^{n}\\;.\n\\] Since we are allowing \\(n\\) to grow large in our application, we approximate\n\\[\nu \\approx e^{-c(1-u)}\\;.\n\\] So, now we have a description of the fraction of nodes that aren’t in the giant component. We can get a description of how many nodes are in the giant component by substituting \\(s = 1-u\\), after which we get the equation we’re really after: \\[\ns = 1- e^{-cs}\n\\qquad(2.2)\\]\nThis equation doesn’t have a closed-form solution for \\(s\\), but we can still plot it and compare the result to simulations (Figure 2.3). Not bad!\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport networkx as nx\n\n# experiment: compute the size of the largest connected \n# component as a function of graph size for a range of mean degrees. \n\ndef largest_component(n, p):\n    G = nx.fast_gnp_random_graph(n, p)\n    S = max(nx.connected_components(G), key=len)\n    return len(S) / n\n\nn = 50000\nC = np.repeat(np.linspace(0.5, 1.5, 11), 10)\nU = np.array([largest_component(n, c/(n-1)) for c in C])\n\n# theory: prediction based on Newman 11.16\n\nS = np.linspace(-.001, .6, 101)\nC_theory = -np.log(1-S)/S\n\n# plot the results to compare\n\nplt.plot(C_theory, \n         S, \n         color = \"black\", \n         label = \"Theoretical prediction\")\n\nplt.scatter(C, \n            U, \n            label = \"Experiment\")\n\nplt.gca().set(xlabel = \"Mean degree\", \n              ylabel = \"Proportion of graph in largest component\")\n\nplt.legend()\n\n\n\n\n\nFigure 2.3: Each point gives the fraction of an ER graph with 50,000 nodes occupied by the largest component. The mean degree is on the horizontal axis. The black line gives the theoretical prediction of Equation 2.2."
  },
  {
    "objectID": "chapters/degree_sequences.html",
    "href": "chapters/degree_sequences.html",
    "title": "3  Random Graphs: Degree Sequences",
    "section": "",
    "text": "A limitation of the Erdős–Rényi model is that it always generates graphs whose degree distributions are approximately Poisson. This isn’t very flexible, and most real-world networks have very different-looking degree sequences. In this set of lecture notes, we’ll discuss two more classes of random graph that emphasize the important role played by the degrees of nodes in determining graph structure."
  },
  {
    "objectID": "chapters/degree_sequences.html#the-configuration-model",
    "href": "chapters/degree_sequences.html#the-configuration-model",
    "title": "3  Random Graphs: Degree Sequences",
    "section": "3.1 The Configuration Model",
    "text": "3.1 The Configuration Model\n\n\n\n\n\n\n\nDefinition 3.1 (Configuration Model) A graph with \\(n\\) nodes has degree sequence \\(\\mathbf{k} \\in \\mathbb{Z}^n_+\\) if each node \\(i\\) has degree \\(k_i\\).\nA configuration model random graph with degree sequence \\(\\mathbf{k}\\in \\mathbb{Z}^n_+\\) is a random graph sampled uniformly at random from the set of all graphs with degree sequence \\(\\mathbf{k}\\).\n\n\n\n\nWhile sampling from the Erdős–Rényi model was as easy as flipping \\(\\binom{n}{2}\\) weighted coins, sampling from the configuration model is actually rather challenging. A classical approach for sampling from the configuration model is to perform a stub-matching algorithm. Here’s how it works.See Fosdick et al. (2018) for a comprehensive discussion.\n\nFirst, each node is assigned \\(k_i\\) stubs or half-edges.\nUntil there are no more stubs left:\n\nTwo stubs are selected uniformly at random and matched, creating an edge.\n\n\nThe stub-matching algorithm can produce multi-edges and self-loops, which can cause the graph to not be simple. However, it can be proven (Bollobás 1980) that, when the graph is sparse, the expected number of multi-edges and self-loops does not grow with network size. One can, as a result, show these structures are rare, and can often be ignored in arguments.\n\nMoments of the Degree Sequence\nLet \\(p_k\\) be the proportion of nodes with degree \\(k\\) in a configuration model. Then, \\(p_kn\\) is the number of nodes with degree \\(k\\). The mean degree is \\(\\langle k \\rangle \\triangleq \\sum_{i \\in N} k p_k\\). More generally, the \\(\\ell\\)th moment of the degree sequence is \\(\\langle k^{\\ell} \\rangle \\triangleq \\sum_{i \\in N}k^\\ell p_k\\).\n\n\nConfiguration Models with Constant Moments\nRecall that, in the case of Erdős–Rényi random graphs, we called a sequence of models sparse if the mean degree \\(c\\) did not increase as a function of \\(n\\). Here, we use a similar working definition\n\n\n\n\n\n\nA sequence of configuration model random graphs has constant moments if the moments \\(\\langle k^\\ell \\rangle\\) converge to constants as \\(n\\) grows large, for each finite \\(\\ell\\).\n\n\n\nWe’ll assume that we’re considering configuration models with constant moments. This assumption justifies “large \\(n\\)” reasoning of the same type we did with Erdős–Rényi random graphs.\n\n\nBranching Process Approximation for Giant Components\nRecall that the branching process approximation  in Erdős–Rényi random graphs suggests that the size of the component containing a uniformly sampled node has expected size approximately \\(1 / (1-c)\\), where \\(c\\) is the average degree. On the other hand, when \\(c > 1\\), the branching process approximation suggests that the component containing a uniformly sampled node could be very large indeed.Definition 2.5\nWe can perform a similar kind of analysis for the configuration model. There is only one new thing to figure out.\n\n\n\n\n\n\n\nDefinition 3.2 (Edge-Biased Degree Distribution) In a configuration model random graph, consider the following random process:\n\nFirst, choose an edge uniformly at random.\nThen, take one of the nodes attached to this edge and log its degree \\(K\\).\n\nThe random variable \\(K-1\\) is distributed according to the edge-biased degree distribution of the graph.\n\n\n\n\n\n\n\n\n\n\nExercise: show that the edge-biased degree distribution is \\(q_k \\triangleq \\mathbb{P}(K = k) = \\frac{kp_k}{\\langle k \\rangle}\\).\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Excess degree distribution) \nIn a configuration model random graph, if \\(K\\) is distributed according to the edge-biased degree distribution, the variable \\(H = K-1\\) is distributed according to the excess degree distribution.\n\n\n\n\nThe reason to care about the excess degree distribution is that it answers the following question:\n\nSuppose I follow an edge in a configuration model random graph, arriving at node \\(i\\). How many additional edges are attached to node \\(i\\)?\n\nThis is an important question, because it is precisely the question that we used to determine whether the branching process would die out quickly or continue forever in the case of the Erdős–Rényi random graph. The same heuristic works here:\n\n\n\n\n\n\n\nTheorem 3.1 (Giant component in configuration models) \nThe configuration model with constant moments has a giant component as \\(n\\) grows large if \\(\\mathbb{E}[H] > 1\\).\n\n\n\n\nIndeed, this result can be formally proven by rigorous arguments, although these are beyond our scope here. Molloy and Reed (1995)\n\n\n\n\n\n\nExercise: Show that \\(\\mathbb{E}[H] = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{\\langle k \\rangle}\\). Infer that the criterion \\(\\mathbb{E}[H] > 1\\) in Theorem 3.1 is equivalent to the criterion \\[\n\\langle k^2 \\rangle - 2\\langle k \\rangle > 0\\;.\n\\]\n\n\n\n\n\nFriendship Paradox\nThe “friendship paradox” refers to the observation which is commonly phrased as:\n\nYour friends have more friends than you do.\n\nThis is only true in a statistical sense: on average, in a graph, if you check the degree of each node and compare that to the mean degree of its neighbors, you’ll find that the average neighbor degree is higher than the average original degree.\nThe math here is that the neighbor degree nothing other than the degree of a node that we reached by following an edge. That is, its distribution is the edge-biased degree distribution. The mean of this distribution is \\(\\mu = \\langle k^2 \\rangle/\\langle k \\rangle\\). The Cauchy-Schwartz inequality implies that \\(\\langle k^2 \\rangle \\geq \\langle k \\rangle^2\\), which in turn implies that \\(\\mu \\geq k\\)."
  },
  {
    "objectID": "chapters/interlude_research_survey.html",
    "href": "chapters/interlude_research_survey.html",
    "title": "4  Interlude: A Rapid Survey of Research Questions",
    "section": "",
    "text": "In this short set of lecture notes, we’ll give a very brief tour of some of the most common research questions and methodologies posed in the network science literature. The purpose of these notes is to help you develop your project ideas and connect them with concrete data sources, models, and methodologies.\nThese notes are nonexhaustive. If you’re interested in a topic or methodology not covered here, that’s ok! Network science is a big place. Feel free to ask me about your ideas and we’ll chat."
  },
  {
    "objectID": "chapters/interlude_research_survey.html#network-data-science",
    "href": "chapters/interlude_research_survey.html#network-data-science",
    "title": "4  Interlude: A Rapid Survey of Research Questions",
    "section": "4.1 Network Data Science",
    "text": "4.1 Network Data Science\nIn many research settings, we aim to perform a data analysis or prediction task using a real-world network data set.\n\nCommunity Detection\nThe community detection (also called graph clustering) task asks us to partition a graph into clusters of related nodes. We’ve already talked a bit about community detection using the minimum-cut methodology, and we’ll see more sophisticated methods in Section 6 (see also Chapter 14 in Newman).\n\n\nRanking and Centrality\nRanking and centrality measures are often used to determine nodes that are important, central, powerful, or influential in a graph. We saw examples of centrality measures based on counting walks in Section 1: betweenness centrality and Katz centrality.\nWe often talk about centrality in undirected graphs and rank in directed graphs, although this is not a hard-and-fast distinction.\nNewman 7.1 has discussions of several types of centrality in networks.\n\n\nLink Prediction\nIn the link prediction problem, we are given some partial information about the edges in a network, and we would like to make a prediction about edges that we haven’t yet observed. For example, companies like Facebook and Twitter would love to know how to predict which users will become friends or followers of other users.\nThere are many ways to perform link prediction. A very classical pipeline looks like this:\n\nFor each node, compute some array of features. Examples of features commonly used in network analysis include:\n\nAny metadata attached to the node (demographics, expressed preferences).\nThe degree of the node.\nThe centrality of the node, using any centrality method.\nAverage demographics of the neighbors of the node.\nAnd lots more!\n\nUsing this array of features, train a statistical or machine learning model on all pairs of nodes to classify whether an edge is present or not, using the computed features.\nFinally, use this model to make predictions about unseen possible edges.\n\nLink prediction is often used as a way to “validate” other kinds of network data science tasks, such as community detection and centrality. When clusters or centrality scores are predictive of future or unseen edges, this can be a reasonable sign that those measures give real insight into the structure or function of the network.\n\n\nNode Attribute Prediction\nIn a rather infamous study, Jernigan and Mistree (2009) found that publicly available information on Facebook could be used to predict sexual orientation. The idea is very simple: they found that gay male users were likely to be friends with other gay male users, and therefore the percentage of a male user’s friends who identify as gay men can be used to predict the orientation of the user himself. This study raises grave concerns about the effective involuntary disclosure of one’s sexual orientation through social networks. Such disclosure can have consequences for one’s social life, political rights, or ability to live safely in many parts of the world.\nFrom a technical standpoint, this study was an early and influential example of node attribute prediction. In the node attribute prediction problem, the aim is to predict some unknown information about a node, given information about some other nodes in the network. Often, a good way to do this is by computing average values of features of nodes connected to the target node. Graph neural networks are also an increasingly popular way to perform node attribute prediction tasks.\n\n\nWhere to get data?\nI’ve listed links to several repositories of network data in the appendix. There are lots of other good ways to get data. In many cases, if you see a paper on a topic that interests you, you can find a link to a repository containing the data that they used."
  },
  {
    "objectID": "chapters/interlude_research_survey.html#network-modeling",
    "href": "chapters/interlude_research_survey.html#network-modeling",
    "title": "4  Interlude: A Rapid Survey of Research Questions",
    "section": "4.2 Network Modeling",
    "text": "4.2 Network Modeling\nIn many cases, it may be expensive, unethical, or scientifically impossible to obtain exactly the data that we want. In such cases, we often turn to mathematical or computer models of networks. We can also approach these problems with explicitly mathematical interest, aiming to prove theorems about them in certain special cases. You’ve seen some flavor of this approach in the recent course content on random graphs. A third reason to study mathematical models of networks is that some, such as the stochastic blockmodel which we will study in Section 6, are foundations for important network algorithms.\n\nProcesses Evolving on Networks\n\nOpinion Dynamics\nIn opinion dynamics, nodes have an opinion about some topic which changes in response to interaction with other nodes. The opinion of each node is often either binary (0 or 1, as in the voter model) or real-valued (as in the DeGroot and bounded confidence models).\n\n\nEpidemic Spread\nEpidemic spreading models are similar in some ways to opinion models, with nodes changing states in response to interactions to other nodes. The primary difference is that the states are more complicated. There is a wide range of different modeling assumptions to consider, some of which may be more appropriate for certain kinds of disease. Epidemic models were first considered as differential equations (the perspective emphasized here), but much recent work has studied them on social networks (see Newman, Chapter 16).\nRecently, there’s been a lot of interest in the topic of misinformation spreading. Many techniques from epidemic modeling have been used in this area as well.\n\n\n\nAdaptive Networks\nIn the models described in the previous section, what changes over time is a property of the nodes. In adaptive network models, the structure of the network can also change. Adaptive network models have become very popular in modeling epidemic spread. For example, suppose that when a node becomes infected, it also practices physical distancing. We might model this as a temporary reduction in that nodes’ connectivity to the rest of the network, and would hope that this behavior could reduce overall epidemic severity.\n\n\nAgent-Based Modeling\nMany of the models we study in the context of social networks can be viewed as agent-based models. An agent-based model consists of a set of agents and a collection of rules determining how those agents interact with each other. Many agent-based models assume that the agents are situated in a network, and interact only with their neighbors on the network. Arbitrarily complex decision-making rules can be expressed through the agent-based modeling framework. The Mesa package for Python is one of many frameworks for implementing agent-based models in programming languages."
  },
  {
    "objectID": "chapters/interlude_research_survey.html#fancy-networks",
    "href": "chapters/interlude_research_survey.html#fancy-networks",
    "title": "4  Interlude: A Rapid Survey of Research Questions",
    "section": "4.3 “Fancy” Networks",
    "text": "4.3 “Fancy” Networks\nSo far in this course, we’ve focused on simple, undirected networks. However, lots of networks have more structure than this, and motivate different kinds of analyses.\n\nMultilayer networks have edges of different categories (Kivelä et al. 2014). An example of a multilayer network is an urban transportation network, which might include layers for both streets and subways. Another kind of multilayer network is the social network of UCLA, where edges of different types could represent relationships like friendship, co-membership in clubs, enmity, collaboration on a project, etc. etc.\n\nTemporal networks have edges that evolve over time (Holme and Saramäki 2012). Often, these edges represent timestamped interactions. For example, in a network of email communications, each email was sent at a certain time, and the timing of emails is important to the flow of information through the graph.\nDirected networks have edges which express asymmetric relationships between nodes. For example, in animal behavior networks, an edge could represent a fight between two animals in a colony, with the edge “pointing” at the winner and away from the loser.\n\nSome networks are multilayer, temporal, and directed. For example, consider Twitter.\n\nMultilayer: Multiple types of networked interaction are possible on Twitter, including following, retweeting, blocking, and replying.\nTemporal: Following and retweeting are specific actions that take place at a specific time. The timing of these actions can be important for their interpretation.\nDirected: interactions like following and retweeting are asymmetric – if I follow you, that doesn’t imply that you follow me.\n\nAnother important kind of fancy network is a polyadic or higher-order network. In polyadic networks, edges can join more than two nodes simultaneously. Polyadic networks are most commonly represented using hypergraphs and simplicial complexes. Bick et al. (2021) give a good overview of many of the math and modeling questions associated with these kinds of models."
  },
  {
    "objectID": "chapters/ranking_centrality.html",
    "href": "chapters/ranking_centrality.html",
    "title": "5  Ranking and Centrality",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "chapters/clustering_community.html",
    "href": "chapters/clustering_community.html",
    "title": "6  Clustering and Community Detection",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "chapters/social_responsibility.html",
    "href": "chapters/social_responsibility.html",
    "title": "7  Networks and Social Responsibility",
    "section": "",
    "text": "Bak-Coleman et al. (2021)\nJernigan and Mistree (2009)"
  },
  {
    "objectID": "appendices/additional_resources.html",
    "href": "appendices/additional_resources.html",
    "title": "Appendix A — Network Science Resources",
    "section": "",
    "text": "Aaron Clauset at CU Boulder maintains excellent lecture notes on Network Analysis and Modeling and Biological Networks.\nLectures on Network Systems, a free book by Francesco Bullo at UC Santa Barbara covering a range of important topics related to dynamical systems on networks. The development of the linear algebra toolbox for approaching network problems is clear and of high general utility.\n\nIntroduction to Probability for Data Science, a free book by Stanley Chan at Purdue covering some of the elementary theory of probability as it relates to statistics and machine learning.\nHigh-Dimensional Probability, is a free book by Roman Vershynin at UC Irvine covering a range of topics on the probabilistic foundations of modern, high-dimensional statistics at an advanced level.\nAlgebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning by Jean Gallier and Jocelyn Quaintance is a monumental, free online book covering a wide range of mathematical background useful in machine learning and data science."
  },
  {
    "objectID": "appendices/additional_resources.html#application-domains",
    "href": "appendices/additional_resources.html#application-domains",
    "title": "Appendix A — Network Science Resources",
    "section": "Application Domains",
    "text": "Application Domains\n\nThe book Economic Networks: Theory and Computation by John Stachurski and Thomas J. Sargent. I haven’t personally read this one, but a quick glance looks good."
  },
  {
    "objectID": "appendices/additional_resources.html#data-sets",
    "href": "appendices/additional_resources.html#data-sets",
    "title": "Appendix A — Network Science Resources",
    "section": "Data Sets",
    "text": "Data Sets\n\nThe Colorado Index of Complex Networks (ICON) hosts a large variety of network data sets spanning a large variety of research fields. ICON is curated by the group of Aaron Clauset at CU Boulder.\nThe Stanford Large Network Dataset Collection (SNAP) hosts a wide range of network data sets. SNAP is curated by Jure Leskovec and Andrej Krevl at Stanford University.\nAustin Benson at Cornell hosts a collection of data sets for a range of problems related to graphs and hypergraphs."
  },
  {
    "objectID": "appendices/additional_resources.html#organizations",
    "href": "appendices/additional_resources.html#organizations",
    "title": "Appendix A — Network Science Resources",
    "section": "Organizations",
    "text": "Organizations\n\n“The society of Women in Network Science (WiNS) connects women, trans and non-binary gender network scientists from different races, socioeconomic backgrounds, and nations. The society aims to recognize the work, perspectives and expertise of its members to create bridges between academia, government, and private industry related to network science.”\nWomen in Math (WIM) at the UCLA math department is an informal group of women graduate students, postdocs, faculty, and visitors. We regularly hold lunches, dinners, and other social gatherings with the goal of fostering community and providing support for the women in the department. WIM welcomes cis and trans women, as well as anyone else who would like to try out the space.\n\nWIM has assembled an excellent resource on applying to graduate school and what to expect when you get there."
  },
  {
    "objectID": "appendices/additional_resources.html#python-resources",
    "href": "appendices/additional_resources.html#python-resources",
    "title": "Appendix A — Network Science Resources",
    "section": "Python Computing",
    "text": "Python Computing\n\nCS For All, a website and book developed for brand-new programming learners by the Department of Computer Science at Harvey Mudd College.\nLecture notes and videos from PIC16A, my course on core skills in Python programming and data science.\nA Whirlwind Tour of Python by Jake VanderPlas is an excellent, rapid overview of fundamental Python skills. It is suitable for those who have experience in several other programming languages, or for those who previously learned Python and just need a brush-up.\n\nLecture notes from PIC16B, my course on advanced computational and data science in Python.\n\nThe Python Data Science Handbook by Jake VanderPlas is an excellent and freely-available online resource for practical data science in Python."
  },
  {
    "objectID": "appendices/acknowledgements.html",
    "href": "appendices/acknowledgements.html",
    "title": "Appendix B — Acknowledgements",
    "section": "",
    "text": "Course environment statements in syllabus lightly modified from Chad Topaz.\nGuiding principles inspired by Robert Talbert."
  },
  {
    "objectID": "appendices/acknowledgements.html#content",
    "href": "appendices/acknowledgements.html#content",
    "title": "Appendix B — Acknowledgements",
    "section": "Content",
    "text": "Content\nThis course on network science is inspired by a number of courses in the area, including:\n\nPrevious versions of MATH 168 at UCLA as offered by Mason Porter and Heather Zinn Brooks (now at Harvey Mudd College).\nPatrick Jaillet’s course 6.268: “Network Science and Models” at MIT.\nAaron Clauset’s course “Network Analysis and Modeling” at CU Boulder."
  },
  {
    "objectID": "appendices/acknowledgements.html#website",
    "href": "appendices/acknowledgements.html#website",
    "title": "Appendix B — Acknowledgements",
    "section": "Website",
    "text": "Website\nA previous version of this course website was powered by the bookdown package for R. It is currently powered by the Quarto publishing system."
  },
  {
    "objectID": "appendices/references.html",
    "href": "appendices/references.html",
    "title": "Appendix C — References",
    "section": "",
    "text": "Bak-Coleman, Joseph B, Mark Alfano, Wolfram Barfuss, Carl T Bergstrom,\nMiguel A Centeno, Iain D Couzin, Jonathan F Donges, et al. 2021.\n“Stewardship of Global Collective Behavior.”\nProceedings of the National Academy of Sciences 118 (27):\ne2025764118.\n\n\nBick, Christian, Elizabeth Gross, Heather A Harrington, and Michael T\nSchaub. 2021. “What Are Higher-Order Networks?” arXiv\nPreprint arXiv:2104.11329.\n\n\nBollobás, Béla. 1980. “A Probabilistic Proof of an Asymptotic\nFormula for the Number of Labelled Regular Graphs.” European\nJournal of Combinatorics 1 (4): 311–16.\n\n\nErdős, Paul, and Alfréd Rényi. 1960. “On the Evolution of Random\nGraphs.” Publ. Math. Inst. Hung. Acad. Sci 5 (1): 17–60.\n\n\nFosdick, Bailey K, Daniel B Larremore, Joel Nishimura, and Johan\nUgander. 2018. “Configuring Random Graph Models with Fixed Degree\nSequences.” SIAM Review 60 (2): 315–55.\n\n\nHolme, Petter, and Jari Saramäki. 2012. “Temporal\nNetworks.” Physics Reports 519 (3): 97–125.\n\n\nJernigan, Carter, and Behram FT Mistree. 2009. “Gaydar: Facebook\nFriendships Expose Sexual Orientation.” First Monday.\n\n\nKatz, Leo. 1953. “A New Status Index Derived from Sociometric\nAnalysis.” Psychometrika 18 (1): 39–43.\n\n\nKivelä, Mikko, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir\nMoreno, and Mason A Porter. 2014. “Multilayer Networks.”\nJournal of Complex Networks 2 (3): 203–71.\n\n\nMolloy, Michael, and Bruce Reed. 1995. “A Critical Point for\nRandom Graphs with a Given Degree Sequence.” Random\nStructures & Algorithms 6 (2-3): 161–80.\n\n\nNadakuditi, Raj Rao, and Mark EJ Newman. 2012. “Graph Spectra and\nthe Detectability of Community Structure in Networks.”\nPhysical Review Letters 108 (18): 188701.\n\n\nNewman, Mark. 2018. Networks: An Introduction.\nOxford University Press.\n\n\nRiordan, Oliver, and Nicholas Wormald. 2010. “The Diameter of\nSparse Random Graphs.” Combinatorics, Probability and\nComputing 19 (5-6): 835–926.\n\n\nWatson, Henry William, and Francis Galton. 1875. “On the\nProbability of the Extinction of Families.” The Journal of\nthe Anthropological Institute of Great Britain and Ireland 4:\n138–44.\n\n\nWatts, Duncan J, and Steven H Strogatz. 1998. “Collective Dynamics\nof ‘Small-World’networks.” Nature 393 (6684): 440–42.\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict\nand Fission in Small Groups.” Journal of Anthropological\nResearch 33 (4): 452–73."
  }
]