<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-0.9.165">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Math 168: Introduction to Networks - 2&nbsp; Random Graphs: Erdős–Rényi</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>

  <script src="../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../site_libs/quarto-nav/headroom.min.js"></script>
  <script src="../site_libs/clipboard/clipboard.min.js"></script>
  <meta name="quarto:offset" content="../">
  <script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
  <script src="../site_libs/quarto-search/fuse.min.js"></script>
  <script src="../site_libs/quarto-search/quarto-search.js"></script>
  <link href="../chapters/ranking_centrality.html" rel="next">
  <link href="../chapters/measurement.html" rel="prev">
  <script src="../site_libs/quarto-html/quarto.js"></script>
  <script src="../site_libs/quarto-html/popper.min.js"></script>
  <script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../site_libs/quarto-html/anchor.min.js"></script>
  <link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link id="quarto-text-highlighting-styles" href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <script id="quarto-search-options" type="application/json">{
    "location": "sidebar",
    "copy-button": false,
    "collapse-after": 3,
    "panel-placement": "start",
    "type": "textbox",
    "limit": 20,
    "language": {
      "search-no-results-text": "No results",
      "search-matching-documents-text": "matching documents",
      "search-copy-link-title": "Copy link to search",
      "search-hide-matches-text": "Hide additional matches",
      "search-more-match-text": "more match in this document",
      "search-more-matches-text": "more matches in this document",
      "search-clear-button-title": "Clear",
      "search-detached-cancel-button-title": "Cancel",
      "search-submit-button-title": "Submit"
    }
  }</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body class="floating slimcontent">
<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Math 168: <br><b>Introduction to Networks</b></a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Network Science</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a href="../syllabus/syllabus.html" class="sidebar-item-text sidebar-link">Syllabus</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus/specifications.html" class="sidebar-item-text sidebar-link">Standard Specifications</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus/resources.html" class="sidebar-item-text sidebar-link">Health, Wellbeing, and Equity</a>
  </div>
</li>
    </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a href="../chapters/intro.html" class="sidebar-item-text sidebar-link">Lecture Notes</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/measurement.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Measuring Networks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/random_graphs.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ranking_centrality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Ranking and Centrality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/clustering_community.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Clustering and Community Detection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/social_responsibility.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Networks and Social Responsibility</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/dynamics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dynamics on Networks</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Appendices</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/additional_resources.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Network Science Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/acknowledgements.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/references.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">References</span></a>
  </div>
</li>
    </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#why-random-graphs" class="nav-link active" data-scroll-target="#why-random-graphs"> <span class="header-section-number">2.1</span> Why Random Graphs?</a>
<ul class="collapse">
<li><a href="#random-graphs-as-insightful-models" class="nav-link" data-scroll-target="#random-graphs-as-insightful-models">Random Graphs as Insightful Models</a></li>
<li><a href="#random-graphs-as-mathematical-puzzles" class="nav-link" data-scroll-target="#random-graphs-as-mathematical-puzzles">Random Graphs as Mathematical Puzzles</a></li>
<li><a href="#random-graphs-as-null-hypotheses" class="nav-link" data-scroll-target="#random-graphs-as-null-hypotheses">Random Graphs as Null Hypotheses</a></li>
<li><a href="#random-graphs-and-statistical-inference" class="nav-link" data-scroll-target="#random-graphs-and-statistical-inference">Random Graphs and Statistical Inference</a></li>
</ul></li>
<li><a href="#the-erdősrényi-model" class="nav-link" data-scroll-target="#the-erdősrényi-model"> <span class="header-section-number">2.2</span> The Erdős–Rényi Model</a>
<ul class="collapse">
<li><a href="#clustering-coefficient" class="nav-link" data-scroll-target="#clustering-coefficient">Clustering Coefficient</a></li>
</ul></li>
<li><a href="#sparsity" class="nav-link" data-scroll-target="#sparsity"> <span class="header-section-number">2.3</span> Sparsity</a>
<ul class="collapse">
<li><a href="#clustering" class="nav-link" data-scroll-target="#clustering">Clustering</a></li>
<li><a href="#cycles-and-local-tree-likeness" class="nav-link" data-scroll-target="#cycles-and-local-tree-likeness">Cycles and Local Tree-Likeness</a></li>
<li><a href="#path-lengths" class="nav-link" data-scroll-target="#path-lengths">Path Lengths</a></li>
<li><a href="#a-caveat" class="nav-link" data-scroll-target="#a-caveat">A Caveat</a></li>
</ul></li>
<li><a href="#component-sizes-and-the-branching-process-approximation" class="nav-link" data-scroll-target="#component-sizes-and-the-branching-process-approximation"> <span class="header-section-number">2.4</span> Component Sizes and the Branching Process Approximation</a>
<ul class="collapse">
<li><a href="#application-to-erdősrényi" class="nav-link" data-scroll-target="#application-to-erdősrényi">Application to Erdős–Rényi</a></li>
<li><a href="#the-subcritical-case" class="nav-link" data-scroll-target="#the-subcritical-case">The Subcritical Case</a></li>
<li><a href="#the-giant-component" class="nav-link" data-scroll-target="#the-giant-component">The Giant Component</a></li>
</ul></li>
</ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">
<header id="title-block-header" class="quarto-title-block default">

<div class="quarto-title"><h1 class="title d-none d-lg-block display-7"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></h1></div></header>

<p>A random graph is a probability distribution over graphs. In this set of lecture notes, we’ll begin our exploration of several random graphs.</p>
<section id="why-random-graphs" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="why-random-graphs"><span class="header-section-number">2.1</span> Why Random Graphs?</h2>
<p>Why should we even study random graphs? There are a few reasons!</p>
<section id="random-graphs-as-insightful-models" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-insightful-models">Random Graphs as Insightful Models</h3>
<p>Random graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations.</p>
</section>
<section id="random-graphs-as-mathematical-puzzles" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-mathematical-puzzles">Random Graphs as Mathematical Puzzles</h3>
<p>Many properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices.</p>
</section>
<section id="random-graphs-as-null-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-null-hypotheses">Random Graphs as Null Hypotheses</h3>
<p>Suppose that you compute the global clustering coefficient (<a href="measurement.html#def-global-clustering">Definition&nbsp;<span>1.11</span></a>) of a graph and find it to be <span class="math inline">\(0.31\)</span>. How do we interpret that? Is that high? Low? In this case, we should ask: <em>compared to what?</em> Random graphs allow us one way to make a comparison: that clustering coefficient is “high” if it’s larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear!</p>
<p>In this way, random graphs often serve as the <em>null hypothesis</em>, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests.</p>
</section>
<section id="random-graphs-and-statistical-inference" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-and-statistical-inference">Random Graphs and Statistical Inference</h3>
<p>Many statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference, and we’ll see a few examples later in the course.</p>
</section>
</section>
<section id="the-erdősrényi-model" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-erdősrényi-model"><span class="header-section-number">2.2</span> The Erdős–Rényi Model</h2>
<div class="page-columns page-full"><p>The model of <span class="citation" data-cites="erdHos1960evolution">Erdős and Rényi (<a href="../appendices/references.html#ref-erdHos1960evolution" role="doc-biblioref">1960</a>)</span> is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is <em>mostly</em> understood in its major mathematical properties, and it’s almost never used in statistical inference. </p><div class="no-row-height column-margin column-container"><span class="aside">The ER model is, however, a building block of models that <em>are</em> used in statistical inference. The <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">stochastic blockmodel</a>, for example, is often used for graph clustering and community detection. In its simplest form, it’s a bunch of ER graphs glued together.</span></div></div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-ER" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Erdős–Rényi Random Graph) </strong></span></p>
<p>An <em>Erdős–Rényi random graph</em> with <span class="math inline">\(n\)</span> nodes and connection probability <span class="math inline">\(p\)</span>, written <span class="math inline">\(G(n,p)\)</span>, is a random graph constructed by placing an edge with probability <span class="math inline">\(p\)</span> between each pair of distinct nodes.</p>
</div>
</div>
</div>
</div>
<p>We can imagine visiting each possible pair of edges <span class="math inline">\((i,j)\)</span> and flipping a coin with probability of heads <span class="math inline">\(p\)</span>. If heads, we add <span class="math inline">\((i,j) \in E\)</span>; otherwise, we don’t.</p>
<div class="callout-caution callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Exercise</strong>: Let <span class="math inline">\(i\)</span> be a fixed node in a <span class="math inline">\(G(n,p)\)</span> graph, and let <span class="math inline">\(K_i\)</span> be its (random) degree. Show that <span class="math inline">\(K_i\)</span> has binomial distribution with success probability <span class="math inline">\(p\)</span> and <span class="math inline">\(n-1\)</span> trials.</p>
</div>
</div>
</div>
<div class="callout-caution callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Exercise</strong>: Show that <span class="math inline">\(\mathbb{E}[K_i] = p(n-1)\)</span>.</p>
</div>
</div>
</div>
<section id="clustering-coefficient" class="level3">
<h3 class="anchored" data-anchor-id="clustering-coefficient">Clustering Coefficient</h3>
<p>Recall that both local and global clustering coefficients were defined in terms of a ratio of realized triangles to possible triangles. Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately.</p>
<p>Let’s take the global clustering coefficient (<a href="measurement.html#def-global-clustering">Definition&nbsp;<span>1.11</span></a>). For this, we need to compute the total number of triangles, and the total number of wedges.</p>
<p>How many triangles are there? Well, there are <span class="math inline">\(\binom{n}{3}\)</span> ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is <span class="math inline">\(p^3\)</span>. So, in expectation, there are <span class="math inline">\(\binom{n}{3}p^3\)</span> triangles in the graph.</p>
<p>How many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is <span class="math inline">\(p^2\)</span>. So, the expected number of wedges is <span class="math inline">\(\binom{n}{3}p^2\)</span>. Our estimate for the expected clustering coefficient is that it should be <em>about</em> <span class="math inline">\(p\)</span>, although we have been fast and loose with several mathematical details to arrive at this conclusion.</p>
</section>
</section>
<section id="sparsity" class="level2 page-columns page-full" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sparsity"><span class="header-section-number">2.3</span> Sparsity</h2>
<p>We are very often interested in the <em>sparse</em> Erdős–Rényi model. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-sparse-ER" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 </strong></span></p>
<p>We say that a <span class="math inline">\(G(n,p)\)</span> graph is <strong>sparse</strong> when <span class="math inline">\(p = c/(n-1)\)</span> for some constant <span class="math inline">\(c\)</span>.</p>
</div>
</div>
</div>
</div>
<p>A consequence of sparsity is that <span class="math inline">\(\mathbb{E}[K_i] = c\)</span>; i.e.&nbsp;the expected degree of a node in sparse <span class="math inline">\(G(n,p)\)</span> is constant. When studying sparse <span class="math inline">\(G(n,p)\)</span>, we are almost always interested in the case <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<section id="clustering" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="clustering">Clustering</h3>
<p>What does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be <em>about</em> <span class="math inline">\(p\)</span>, and if <span class="math inline">\(p = c/(n-1)\)</span>, then <span class="math inline">\(p \rightarrow 0\)</span>.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="aside">The need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of <span class="citation" data-cites="watts1998collective">Watts and Strogatz (<a href="../appendices/references.html#ref-watts1998collective" role="doc-biblioref">1998</a>)</span>, a famous paper that introduced the “small world model.”</span></div></div>
<blockquote class="blockquote">
<p>Sparse Erdős–Rényi graphs have vanishing clustering coefficients.</p>
</blockquote>
</section>
<section id="cycles-and-local-tree-likeness" class="level3">
<h3 class="anchored" data-anchor-id="cycles-and-local-tree-likeness">Cycles and Local Tree-Likeness</h3>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-cycle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 </strong></span></p>
<p>A <strong>cycle</strong> is a walk that does not repeat edges and ends at the same node that it begins.</p>
</div>
</div>
</div>
</div>
<p>A triangle is an example of a cycle of length <span class="math inline">\(3\)</span>.</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-rare-cycles" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 </strong></span></p>
<p>In the sparse <span class="math inline">\(G(n,p)\)</span> model, for any length <span class="math inline">\(k\)</span>, the probability that there exists a cycle of length <span class="math inline">\(k\)</span> attached to node <span class="math inline">\(i\)</span> shrinks to 0 as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</div>
</div>
</div>
</div>
<p>You’ll prove a generalization of <a href="#thm-rare-cycles">Theorem&nbsp;<span>2.1</span></a> in an upcoming homework problem.</p>
<p>Graphs in which cycles are very rare are often called <em>locally tree-like</em>. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble.</p>
</section>
<section id="path-lengths" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="path-lengths">Path Lengths</h3>
<div class="page-columns page-full"><p>How far apart are two nodes in <span class="math inline">\(G(n,p)\)</span>? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail.  However, we can get a big-picture view of the situation by asking a slightly different question:</p><div class="no-row-height column-margin column-container"><span class="aside">See <span class="citation" data-cites="riordan2010diameter">Riordan and Wormald (<a href="../appendices/references.html#ref-riordan2010diameter" role="doc-biblioref">2010</a>)</span> and references therein.</span></div></div>
<blockquote class="blockquote">
<p>Given two nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, what is the expected number of paths of length <span class="math inline">\(k\)</span> between them?</p>
</blockquote>
<p>Call the number of <span class="math inline">\(k\)</span>-paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> <span class="math inline">\(R(k)\)</span>. Let <span class="math inline">\(r(k) = \mathbb{E}[R(k)]\)</span>. Let’s estimate <span class="math inline">\(r(k)\)</span>.</p>
<p>First, we know that <span class="math inline">\(r(1) = p\)</span>. For higher values of <span class="math inline">\(k\)</span>, we’ll use the following idea: in order for there to be a path of length <span class="math inline">\(k\)</span> from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, there must be a node <span class="math inline">\(\ell\)</span> such that:</p>
<ul>
<li>There exists a path from <span class="math inline">\(i\)</span> to <span class="math inline">\(\ell\)</span> of length <span class="math inline">\(k-1\)</span>. In expectation, there are <span class="math inline">\(r(k-1)\)</span> of these.</li>
<li>There exists a path from <span class="math inline">\(\ell\)</span> to <span class="math inline">\(j\)</span> of length <span class="math inline">\(1\)</span>. This happens with probability <span class="math inline">\(p\)</span>.</li>
</ul>
<p>There are <span class="math inline">\(n-2\)</span> possibilities for <span class="math inline">\(\ell\)</span> (excluding <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>), and so we obtain the approximate relation</p>
<p><span class="math display">\[
r(k) \approx (n-2)p r(k-1)\;.
\]</span></p>
<div class="callout-warning callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Why is this an approximation? Well, some of the paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span> that are counted in <span class="math inline">\(r(k-1)\)</span> could actually include the edge <span class="math inline">\((j, \ell)\)</span> <em>already</em>. An example is <span class="math inline">\((i,j), (j,\ell)\)</span>. In this case, the presence of edge <span class="math inline">\((j,\ell)\)</span> is not independent of the presence of the path between <span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span>. The derivation above implicitly treats these two events as independent. Again, because <em>cycles are rare in large, sparse ER</em>, this effect is small when <span class="math inline">\(k\)</span> is small.</p>
</div>
</div>
</div>
<p>Proceeding inductively and approximating <span class="math inline">\(n-2 \approx n-1\)</span> for <span class="math inline">\(n\)</span> large, we have the relation</p>
<p><span id="eq-path-expectation"><span class="math display">\[
r(k) \approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p
\qquad(2.1)\]</span></span></p>
<p>in the sparse ER model.</p>
<p>Using this result, let’s ask a new question:</p>
<blockquote class="blockquote">
<p>What path length <span class="math inline">\(k\)</span> do I need to allow to be confident that there’s a path between nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>?</p>
</blockquote>
<p>Well, suppose we want there to be <span class="math inline">\(q\)</span> paths. Then, we can solve <span class="math inline">\(q = c^{k-1}p\)</span> for <span class="math inline">\(k\)</span>, which gives us:</p>
<p><span class="math display">\[
\begin{aligned}
q &amp;= c^{k-1}p \\
\log q &amp;= (k-1)\log c + \log p \\
\log q &amp;= (k-1)\log c + \log c - \log n \\
\frac{\log q + \log n}{\log c} &amp;= k
\end{aligned}
\]</span></p>
<p>Here, I’ve also approximated <span class="math inline">\(\log n-1 \approx \log n\)</span> for <span class="math inline">\(n\)</span> large.</p>
<p>So, supposing that I want there to be at least one path in expectation (<span class="math inline">\(q = 1\)</span>), I need to allow <span class="math inline">\(k = \frac{\log n}{\log c}\)</span>. This is pretty short, actually! For example, the population of the world is about <span class="math inline">\(8\times 10^9\)</span>, and Newman estimates that an average individual knows around 1,000 other people; that is, <span class="math inline">\(c = 10^3\)</span> in the world social network. This seems a little high to me (maybe I’m antisocial), but the resulting value of <span class="math inline">\(k\)</span> here is around 3.3.</p>
<p>In other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than <span class="math inline">\(4\)</span>.</p>
<p>More formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of <span class="math inline">\(n\)</span>, even in relatively sparse cases.</p>
</section>
<section id="a-caveat" class="level3">
<h3 class="anchored" data-anchor-id="a-caveat">A Caveat</h3>
<p>If you spend some time looking at <a href="#eq-path-expectation">Equation&nbsp;<span>2.1</span></a>, you might find yourself wondering:</p>
<blockquote class="blockquote">
<p>Hey, what happens if <span class="math inline">\(c \leq 1\)</span>?</p>
</blockquote>
<p>Indeed, something <em>very</em> interesting happens here. Let’s assume <span class="math inline">\(c &lt; 1\)</span> (i.e.&nbsp;we’re ignoring the case <span class="math inline">\(c = 1\)</span>), and estimate the expected number of paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of <em>any</em> length. Using <a href="#eq-path-expectation">Equation&nbsp;<span>2.1</span></a>, we get</p>
<p><span class="math display">\[
\mathbb{E}\left[\sum_{k = 1}^{\infty} R(k)\right] = \sum_{k = 1}^\infty c^{k-1}p = \sum_{k = 0}^\infty c^kp = \frac{p}{1-c}\;.
\]</span></p>
<p>If we now use Markov’s inequality, we find that the probability that there is a path of <em>any</em> length between nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is no larger than</p>
<p><span class="math display">\[
\frac{p}{1-c} = \frac{c}{(1-c)(n-1)}\rightarrow 0\;.
\]</span></p>
<p>So, this suggests that, if <span class="math inline">\(c &lt; 1\)</span>, any two nodes are likely to be disconnected! On the other hand, if <span class="math inline">\(c &gt; 1\)</span>, we’ve argued that we can make <span class="math inline">\(k\)</span> large enough to have high probability of a path of length <span class="math inline">\(k\)</span> between those nodes.</p>
<p>So, what’s special about <span class="math inline">\(c = 1\)</span>? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let’s study in a bit more detail the sizes of the <em>connected components</em> of the ER graph.</p>
</section>
</section>
<section id="component-sizes-and-the-branching-process-approximation" class="level2 page-columns page-full" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="component-sizes-and-the-branching-process-approximation"><span class="header-section-number">2.4</span> Component Sizes and the Branching Process Approximation</h2>
<p>We’re now going to ask ourselves about the size of a “typical” component in the Erdős–Rényi model. In particular, we’re going to be interested in whether there exists a component that fills up “most” of the graph, or whether components tend to be vanishingly small in relation to the overall graph size.</p>
<p>Our first tool for thinking about this question is the <em>branching process approximation.</em> Informally, a branching process is a process of <em>random generational growth</em>. We’ll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram:</p>
<div class="cell page-columns page-full" data-hash="random_graphs_cache/html/unnamed-chunk-2_37588aa65e8a1cee65f4b6d2990a2041">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="http://1.bp.blogspot.com/-Y59SK92Nd0c/VIoqelzfc8I/AAAAAAAAAdA/XAQB5yDStUc/s1600/eg1.PNG" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption margin-caption"><a href="http://mytechroad.com/markov-chain-branching-process/">Image source</a>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We start with a single entity, <span class="math inline">\(X_0\)</span>. Then, <span class="math inline">\(X_0\)</span> has a random number of “offspring”: <span class="math inline">\(X_1\)</span> in total. Then, each of those <span class="math inline">\(X_1\)</span> offspring has some offspring of their own; the total number of these offspring is <span class="math inline">\(X_2\)</span>. The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process “dies out.”</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Some of this exposition in this section draws on <a href="https://www.stat.berkeley.edu/users/aldous/Networks/lec2.pdf">these notes</a> by David Aldous.</p>
</div></div><div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-branching-process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Branching Process) </strong></span>Let <span class="math inline">\(p\)</span> be a probability distribution on <span class="math inline">\(\mathbb{Z}\)</span>, called the <strong>offspring distribution</strong>.</p>
<p>A <strong>branching process</strong> with distribution <span class="math inline">\(p\)</span> is a sequence of random variables <span class="math inline">\(X_0, X_1,,X_2\ldots\)</span> such that <span class="math inline">\(X_0 = 1\)</span> and, for <span class="math inline">\(t \geq 1\)</span>,</p>
<p><span class="math display">\[
X_t = \sum_{i = 1}^{X_{t-1}} Y_i\;,
\]</span></p>
<p>where each <span class="math inline">\(Y_i\)</span> is distributed i.i.d. according to <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Technically, this is a <em>Galton-Watson</em> branching process, named after the two authors who first proposed it <span class="citation" data-cites="watson1875probability">(<a href="../appendices/references.html#ref-watson1875probability" role="doc-biblioref">Watson and Galton 1875</a>)</span>. <br> <br> <strong>History note</strong>: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames.</p>
</div></div><section id="application-to-erdősrényi" class="level3">
<h3 class="anchored" data-anchor-id="application-to-erdősrényi">Application to Erdős–Rényi</h3>
<p>Branching processes create <em>trees</em> – graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that <em>cycles are rare in Erdős–Rényi random graphs</em>. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well.</p>
<p>Here’s the particular form of the branching process approximation that we will use:</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-branching-approx" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Branching Process Approximation for ER Component Sizes) </strong></span>Sample a single node <span class="math inline">\(j\)</span> at random from a large, sparse ER graph with mean degree <span class="math inline">\(c\)</span>, and let <span class="math inline">\(S\)</span> be the size (number of nodes) of the component in which <span class="math inline">\(j\)</span> lies. Note that <span class="math inline">\(S\)</span> is random: it depends both on <span class="math inline">\(j\)</span> and on the realization of the ER graph.</p>
<p>Then, <span class="math inline">\(S\)</span> is distributed approximately as <span class="math inline">\(T\)</span>, where <span class="math inline">\(T = \sum_{i = 0}^{\infty}X_t\)</span> is the total number of offspring in a GW branching process with offspring distribution <span class="math inline">\(\text{Poisson}(c)\)</span>.</p>
</div>
</div>
</div>
</div>
<p>The idea behind this approximation is:</p>
<ul>
<li>We start at <span class="math inline">\(j\)</span>, which has <span class="math inline">\(\text{Poisson}(c)\)</span> (as you proved in HW).</li>
<li>Each of these neighbors also has <span class="math inline">\(\text{Poisson}(c)\)</span> <em>new</em> neighbors, and so on.</li>
<li>We keep visiting new neighbors until we run out, and add up the number of neighbors we’ve visited to obtain <span class="math inline">\(S\)</span>.<br>
</li>
<li>Since <em>cycles are rare in ER</em>, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process <em>also</em> approximately describes <span class="math inline">\(T\)</span> in a branching process with a <span class="math inline">\(\text{Poisson}(c)\)</span> offpsring distribution.</li>
</ul>
<div class="callout-important callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Exercise</strong>: In the second generation, we get to a new node by following an edge. For that reason, shouldn’t the number of <em>new</em> edges be <span class="math inline">\(\text{Poisson}(c-1)\)</span> rather than <span class="math inline">\(\text{Poisson}(c-1)\)</span>? Why or why not?</p>
</div>
</div>
</div>
</section>
<section id="the-subcritical-case" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-subcritical-case">The Subcritical Case</h3>
<p>The mean of a <span class="math inline">\(\text{Poisson}(c)\)</span> random variable is again <span class="math inline">\(c\)</span>. As you’ll show in homework, this implies that <span class="math inline">\(X_t\)</span>, the number of offspring in generation <span class="math inline">\(t\)</span>, satisfies <span class="math inline">\(\mathbb{E}[X_t] = c^{t}\)</span>. It follows that, when <span class="math inline">\(c &lt; 1\)</span>, <span class="math inline">\(\mathbb{E}[T] = \frac{1}{1-c}\)</span>.</p>
<p>Now using Markov’s inequality, we obtain the following results:</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="math inline">\(\text{Poisson}(c)\)</span> branching process with <span class="math inline">\(c &lt; 1\)</span>, <span class="math display">\[\mathbb{P}(X_t &gt; 0) \leq c^t\;.\]</span></p>
</div>
</div>
</div>
<p>So, the probability that the branching process hasn’t yet “died out” decays exponentially with timestep <span class="math inline">\(t\)</span>. In other words, the branching process becomes very likely to die out very quickly.</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="math inline">\(\text{Poisson}(c)\)</span> branching process with <span class="math inline">\(c &lt; 1\)</span>, <span class="math display">\[\mathbb{P}(T &gt; a) \leq \frac{1}{a}\frac{1}{1-c}\]</span></p>
</div>
</div>
</div>
<p>In particular, for <span class="math inline">\(a\)</span> very large, we are guaranteed that <span class="math inline">\(\mathbb{P}(T &gt; a)\)</span> is very small.</p>
<div class="page-columns page-full"><p>Summing up, when <span class="math inline">\(c &lt; 1\)</span>, the GW branching process dies out quickly and contains a relatively small number of nodes: <span class="math inline">\(\frac{1}{1-c}\)</span> in expectation. </p><div class="no-row-height column-margin column-container"><span class="aside">In this setting, the branching process is called <em>subcritical</em>.</span></div></div>
<section id="back-to-er" class="level4">
<h4 class="anchored" data-anchor-id="back-to-er">Back to ER</h4>
<p>If we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic:</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Heuristic</strong>: In a sparse ER random graph with mean <span class="math inline">\(c &lt; 1\)</span>, the expected size of a component containing a randomly selected node is roughly <span class="math inline">\(\frac{1}{1-c}\)</span>.</p>
<p>In particular, since this quantity is independent of <span class="math inline">\(n\)</span>, we find that the <em>fraction</em> of the graph occupied by this component is <span class="math inline">\(\frac{1}{n}\frac{1}{1-c}\)</span> and therefore vanishes as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
</div>
</div>
</div>
<p>For this reason, we say that subcritical ER contains only <em>small</em> connected components, in the sense that each component contains approximately 0% of the graph as <span class="math inline">\(n\)</span> grows large.</p>
<p>This explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are <em>on the same connected component</em>. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes.</p>
</section>
</section>
<section id="the-giant-component" class="level3">
<h3 class="anchored" data-anchor-id="the-giant-component">The Giant Component</h3>



</section>
</section>
</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    setTimeout(function() {
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/measurement.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Measuring Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/ranking_centrality.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Ranking and Centrality</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->


</body></html>