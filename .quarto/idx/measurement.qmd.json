{"title":"Measuring Networks","markdown":{"headingText":"Measuring Networks","headingAttr":{"id":"measurement","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n*These lecture notes are based on Chapters 6 and 7 of Newman. They are a short set of highlights, and are not a substitute for actually reading these chapters! There will be content not covered in these notes that you'll need for homework problems.* \n\n## Networks and Matrices\n\nFormally, an **undirected graph** $G = (N, E)$ is a set of nodes $N$ and a set of edges $E\\subseteq N\\times N$. Each element of $E$ is an *unordered* pair of nodes in $N$. We'll focus on the case of *simple graphs,* in which there are no multi-edges or self-loops (see Newman 6.2 for discussion of these cases).  \n\nMatrices are fundamental tools for studying networks. Why is that? The key point is that a graph is a collection of pairwise relationships encoded by $E$, and matrices are really good for describing pairwise relationships! \n\n### The Adjacency Matrix \n\nEasily the most fundamental of the matrices associated to a graph $G$ is the adjacency matrix $\\mathbf{A}$, with entries\n\n$$\na_{ij} =\n\\begin{cases}\n    1 &\\quad (i,j) \\in E \\\\ \n    0 &\\quad \\mathrm{otherwise.}\n\\end{cases}\n$$\n\nThe reason the adjacency matrix is so important is that it is a lossless representation of the graph structure -- given knowledge of $\\mathbf{A}$, you can fully reconstruct the graph. Not all matrices have this property. \n\n#### Walks {.unnumbered}\n\nA *walk of length $k \\geq 2$* in a graph is a set of edges $(i_1,j_1), (i_2,j_2), \\ldots, (i_k,j_k)$ with the property that $i_\\ell = j_{\\ell-1}$ for each $2 \\leq \\ell \\leq k$. This definition doesn't work for $k = 1$; by convention, a single edge $(i,j)$ is always considered a walk of length 1. \n\nA question that pops up a lot in network analysis is: \n\n> How many walks of length $k$ exist between nodes $i$ and $j$? \n\nThe adjacency matrix gives a concise way to address this question. First, let's consider $k = 1$. That's just the number of edges between nodes $i$ and $j$, which is exactly $a_{ij}$. Said another way, \n\n> The $ij$ th entry of $\\mathbf{A}^1$ counts the number of walks of length $1$ between nodes $i$ and $j$. \n\nThis turns out to generalize smoothly by induction. Let's try out this argument now. \n\nSuppose that $\\mathbf{W}(k)$ is a matrix whose entry $w_{ij}(k)$ contains the number of walks between nodes $i$ and $j$ of length $k$. Then, $\\mathbf{W}(k+1) = \\mathbf{W}(k)\\mathbf{A}$ has entries $w_{ij}(k+1)$ containing the number of walks of length $k+1$. \n\nThe proof is fast: we just expand out the matrix product and interpret each term: \n\n\n\n$$[\\mathbf{W}(k)\\mathbf{A}]_{ij} = \\sum_{\\ell \\in N}w_{i\\ell}(k)a_{\\ell j}\\;.$$\n\n::: {.column-margin}\ncf. Newman's eq. 6.22\n:::\n\n**Exercise**: What's a *very fast* argument that this sum does indeed express the number of walks of length $k+1$ from $i$ to $j$? \n\n### A Linear Algebra Interlude {.unnumbered}\n\nWhat kind of information does the matrix $\\mathbf{A}$ hold about the graph? Well, one answer is \"all of it,\" because $\\mathbf{A}$ determines the graph up to permutations of node labels. But there's a more useful answer as well. When we as about the information contained in a matrix, we often look at the *eigenvalues* and *eigenvectors.* The eigenvalues and eigenvectors of the adjacency matrix can contain some useful information about the graph structure. Let's see an example. \n\nLet \n\n$$\\mathbf{K}_n = \\left[\\begin{matrix}\n    0 & 1 & 1 & \\cdots & 1 \\\\\n    1 & 0 & 1 & \\cdots & 1 \\\\\n    1 & 1 & 0 & \\cdots & 1 \\\\\n    \\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\ \n    1 & 1 & 1 & 0 \\cdots \n\\end{matrix}\\right]\\;.$$\n\nThere are $n$ rows and $n$ columns. \n\nThe is the adjacency matrix of an *n-clique*: a graph on $n$ nodes in which all nodes are connected to each other. \n\nLet's now consider the matrix \n$$\\mathbf{A}_{2n} = \\left[\\begin{matrix} \n    \\mathbf{K}_n & \\mathbf{I}_n \\\\ \n    \\mathbf{I}_n & \\mathbf{K}_n \n\\end{matrix}\\right]\\;.$$\n\nHere, $\\mathbf{I}_n$ is the $n\\times n$ identity matrix. \n\nNow, $\\mathbf{A}_n$ is the matrix of two cliques that have been \"paired\", with each node in one clique connected to exactly one node in the other clique. It looks a bit like this: \n\n```{r}\n#| message : FALSE \n#| warning : FALSE \n#| fig.align : 'center'\n#| out.width : 50%\n#| cap-location: margin\n#| fig.cap : \"A visualization of the 'paired cliques' example with 20 total nodes. Nodes in the first clique are represented by orange circles, and nodes in the second clique by blue squares.\"\n\nknitr::include_graphics(\"img/paired-cliques.png\")\n```\n\nWhat kinds of information are contained in the first few eigenvectors of $\\mathbf{A}_{2n}$? \n\n**Exercise**: The vector of ones $\\mathbf{1}_{2n}$ is an eigenvector of $\\mathbf{A}_{2n}$. What is its eigenvalue? How do we know whether it is the largest one?   \n\n**Exercise**: The vector $\\mathbf{v} = (\\mathbf{1}_n, - \\mathbf{1}_n)$ is another eigenvector of $\\mathbf{A}_{2n}$. What is its eigenvalue? \n\nIn fact, it's true that these are the two largest eigenvalues of $\\mathbf{A}$. The first one isn't very interesting, but note that the second one actually separates the two cliques! \n\nSo, suppose we were given a graph where: \n\n- We knew that the graph had the paired-clique structure, but \n- We didn't know which node belonged to which clique. \n\nA way to solve this problem would be to compute the *second eigenvector* $\\mathbf{v}$. The signs of $\\mathbf{v}$ separate the two cliques. This idea is the foundation of many *spectral graph clustering* algorithms. \n\nIn fact, the adjacency matrix isn't usually the optimal matrix to use for spectral algorithms.[@nadakuditi2012graph] This is a deep and important story related to *random matrix theory*, which has many connections to network science. \n\n### Degrees {.unnumbered}\n\nThe *degree* of a node is the number of edges attached to it: \n\n$$k_i = \\left|\\left\\{j:(i,j) \\in E\\right\\}\\right|\\;.$$\n\nThe degree is a fundamental quantity in many network analyses. Especially the *distribution of degrees* in the network can play a major role in both theory and applications. \n\n**Exercise**: show that the diagonal entries of $\\mathbf{A}^2$ give the degree of each node. \n\nWe often collect the degrees into a diagonal matrix $\\mathbf{D}$ whose diagonal entry $d_{ii} = k_i$ contains the degree of node $i$. \n\n### The Laplacian {.unnumbered}\n\nAnother very important matrix for network representations is the *graph Laplacian matrix*. Actually, there are multiple matrices with claim to this name, but the one we'll usually focus on is the *combinatorial Laplacian* $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$. \n\n**Exercise**: Given knowledge of the combinatorial Laplacian $\\mathbf{L}$, is it possible to exactly reconstruct the graph?\n\nThe Laplacian is often used to represent *(diffusive) flows* of quantities between nodes. To see why, suppose that I have some amount of water $x_i$ on each node $i$, and that I collect this into a vector $\\mathbf{x}$. Now, consider the vector $\\mathbf{L}\\mathbf{x}$. \n\n$$\n\\begin{align}\n[\\mathbf{L}\\mathbf{x}]_i &= \\sum_{j} \\left(d_{ij}x_j - a_{ij}x_j \\right) \\\\ \n&= \\underbrace{k_ix_i}_{\\text{flow out of }i} - \\underbrace{\\sum_{j} a_{ij}x_j}_{\\text{flow into }i}\\;.\n\\end{align}$$\n\nThe first term distributes the water $x_i$ at node $i$ to each of $i$'s $k_i$ neighbors, while the second term allows water to flow into node $i$ from each neighbor along each edge connecting them.\n\nModifications of this setup will become very important when we discuss random walks later in the course. \n\n\n### Many More Matrices... {.unnumbered}\n\nThere are LOTS of matrices that can be associated to networks. There's no \"right\" one -- some are more useful than others for certain jobs. Throughout this course, we'll see examples of matrices that are well-suited to certain specific tasks, like ranking or clustering. If you're interested in searching around a bit, some other fun matrices are: \n\n- The *nonbacktracking* or *Hashimoto* matrix. \n- The modularity matrix. \n- The random-walk matrix. \n- The random-walk and symmetric normalized Laplacian matrices. \n- The PageRank matrix. \n- The node-edge incidence matrix.\n\nAnd the list goes on! \n\n\n### Directed and Weighted Graphs {.unnumbered}\n\nNewman Chapter 6 contains a nice introductory discussion of directed and weighted graphs. We won't spend a lot of time on these at this stage of the course, but it's worthwhile reading this material as it may be of interest as you think about projects. \n\n## Measures and Metrics {.unnumbered}\n\n### Local Measures {.unnumbered}\n\n#### Node Importance  {.unnumbered}\n\n- Degree\n- Betweenness centrality\n- Clustering coefficients \n- Degree distributions\n\n\n### Global Measures {.unnumbered}\n\n#### Network Size {.unnumbered}\n\n- Number of nodes \n- Edge density \n- Diameter \n\n#### Cluster Structure {.unnumbered} \n\n- Cut sizes\n\n#### Hierarchy/Planarity {.unnumbered} \n\n- Linearity, minimum-violation ranking\n\n\n### Looking Ahead {.unnumbered}\n\n> An important question to ask about many of these metrics is: *what counts as a large, surprising, or meaningful* value of a given metric? \n\nOne way to address this question is to take some value and compare it to one that we believe to be small, unsurprising, or not meaningful. We often operationalize this by saying that \"a random graph\" would have some property, such as a low clustering coefficient. So, a high clustering coefficient is surprising, and suggests that the graph might not be \"random.\" This motivation takes directly to the study of random graphs, which is one of the mathematical foundations of network science. \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"self-contained-math":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"measurement.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"bibliography":["references.bib"],"theme":"cosmo","number-depth":2},"extensions":{"book":{"multiFile":true}}}}}